<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="http://localhost:4000/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.4.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-09-08T16:12:26+08:00</updated><id>http://localhost:4000//</id><title type="html">Hi, I’m Wang qinxiao</title><subtitle>读有趣的书，做有趣的事。
</subtitle><author><name>by 王勤晓</name></author><entry><title type="html">决策树算法 ID3, C4.5, CART</title><link href="http://localhost:4000/jekyll/update/2019/09/07/ID3.html" rel="alternate" type="text/html" title="决策树算法 ID3, C4.5, CART" /><published>2019-09-07T18:30:00+08:00</published><updated>2019-09-07T18:30:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/09/07/ID3</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/09/07/ID3.html">&lt;p&gt;决策树通过不断的特征二分决策，给出样本的类别。在这一过程中，有三个核心步骤：1.节点（特征，也称属性）的选择及节点顺序。2.节点如何划分。3.剪枝，剔除多余的分支，形成最终的树。决策树经典算法ID3，C4.5和CART的主要区别在第一个步骤，即特征的选择和使用顺序上用到的算法不同。&lt;/p&gt;

&lt;h2 id=&quot;id3-基于信息增益&quot;&gt;ID3-基于信息增益&lt;/h2&gt;
&lt;p&gt;ID3（Iterative Dichotomiser 3，迭代二分器三代）由罗斯昆（J.R.Quinlan）1975年在悉尼大学提出了ID3算法。该算法基于香农（Shannon C E.）在信息论中提出的信息熵的概念。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;信息熵表示信息量的度量，熵越大，表示可能发生的结果越多，不确定性越高。在分类场景中，对于一堆样本数据，如果信息熵高说明种类多，纯度越低，相反则种类少，纯度越高。对于特征数据，信息熵越小说明该特征分类后的纯度越高，对分类系统越有效。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;为了更直观的理解，可将信息熵转化为信息增益。ID3算法将特征的信息增益作为特征选取顺序的依据。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;信息增益表示某个特征能够为分类带来多少“信息”。信息增益越高，表示该特征为分类提供的信息越多。如计算瓜蒂形状的信息增益 ＝ 样本数据总的信息熵 － 瓜蒂分类后的信息熵。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;信息熵，信息增益的公式如下：
&lt;img src=&quot;http://localhost:4000/assets/images/INFORMATIONH.png&quot; alt=&quot;INFORMATIONH&quot; /&gt;&lt;/p&gt;

&lt;p&gt;关于信息熵和信息增益，推荐&lt;a href=&quot;https://www.jianshu.com/p/69dbb042a0e3&quot;&gt;https://www.jianshu.com/p/69dbb042a0e3&lt;/a&gt;中例子，很直观。&lt;/p&gt;

&lt;p&gt;ID3使用信息增益来进行特征的选择，存在的问题是，属性值多的指标（如身份证号）往往信息增益会大，因此在选择时对于多属性值的特征有偏好。&lt;/p&gt;

&lt;h2 id=&quot;c45基于信息增益率&quot;&gt;C4.5－基于信息增益率&lt;/h2&gt;
&lt;p&gt;罗斯昆在自己的ID3算法基础上进行扩展，提出了C4.5算法。C4.5也是基于信息熵的概念，与ID3不同是，使用了信息增益率。信息增益率在评估特征时，把属性值的数量也考虑了进去，从而避免了属性值过多影响判断的问题。&lt;/p&gt;

&lt;p&gt;特征属性值数目越多，则IV(a)的值越大，信息增益率越小，一定程度上削弱了“对属性值多的特征”的偏好，但是同时也可能出现“对属性值少的特征”的偏好的问题。&lt;/p&gt;

&lt;h2 id=&quot;c45基于基尼指数&quot;&gt;C4.5－基于基尼指数&lt;/h2&gt;

&lt;p&gt;CART（Classification And Regression Tree，分类回归树）Leo Breiman, Jerome Friedman, Richard Olshen与Charles Stone于1984年提出。CART和ID3、C4.5算法不同的之处在于：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;ID3、C4.5算法都是基于信息熵来进行划分节点选取，主要用于分类问题，分类和回归问题都可以使用。本文直讨论分类。&lt;/li&gt;
  &lt;li&gt;在节点划分时，只做二元划分。比如特征颜色取值红、黄、蓝，ID3和C4.5直接就划分为红、黄、蓝三个子类，而CART会划分为｛红｝和｛黄、蓝｝，或者｛红，黄｝和｛蓝｝，穷举所有组合。这样的好处是可以避免特征属性值数目多少的偏好问题。但准确率会有所降低。&lt;/li&gt;
  &lt;li&gt;CART划分的标准是使用基尼指数。当样本越不均匀，基尼指数越小。因此，我们可以先选取划分后基尼指数最小的特征（最能够区分样本）进行划分。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;基尼指数公式如下：
&lt;img src=&quot;http://localhost:4000/assets/images/GINI.SVG&quot; alt=&quot;GINI&quot; /&gt;&lt;/p&gt;

&lt;p&gt;例如，一个节点N划分后，子节点a中类0为1个，类1为5个，子节点b中类0为4个，类1为1个，则Gini(N
) ＝ |a|/|N|&lt;em&gt;Gini(a) + |b|/|N|&lt;/em&gt;Gini(b) = 6/11&lt;em&gt;(1-((1/6)&lt;/em&gt;(1/6)+(5/6)&lt;em&gt;(5/6))) + 5/11&lt;/em&gt;(1-((4/5)&lt;em&gt;(4/5)+(1/5)&lt;/em&gt;(1/5)))&lt;/p&gt;

&lt;p&gt;———本文结束&lt;/p&gt;

&lt;p&gt;［参考文档］&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jianshu.com/p/69dbb042a0e3&quot;&gt;https://www.jianshu.com/p/69dbb042a0e3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/27313529&quot;&gt;https://zhuanlan.zhihu.com/p/27313529&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;《数据挖掘导论》&lt;/li&gt;
&lt;/ol&gt;</content><author><name>by 王勤晓</name></author><summary type="html">决策树通过不断的特征二分决策，给出样本的类别。在这一过程中，有三个核心步骤：1.节点（特征，也称属性）的选择及节点顺序。2.节点如何划分。3.剪枝，剔除多余的分支，形成最终的树。决策树经典算法ID3，C4.5和CART的主要区别在第一个步骤，即特征的选择和使用顺序上用到的算法不同。</summary></entry><entry><title type="html">数据挖掘算法－决策树</title><link href="http://localhost:4000/jekyll/update/2019/09/01/DecisionTree..html" rel="alternate" type="text/html" title="数据挖掘算法－决策树" /><published>2019-09-01T21:30:00+08:00</published><updated>2019-09-01T21:30:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/09/01/DecisionTree.</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/09/01/DecisionTree..html">&lt;p&gt;决策树（Decision Tree）是十大经典数据挖掘／机器学习算法，用于分类和回归。本文主要讨论分类。&lt;/p&gt;

&lt;p&gt;决策树算法用于将样本进行分类（通常二分类）。例如，著名的西瓜例子，根据瓜蒂形状、瓜皮颜色、敲瓜声音等特征，反复进行二分决策（瓜蒂卷的是好瓜，否则看瓜皮-瓜皮深的是好瓜，否则再听声音……），从而分类好瓜还是坏瓜。该算法决策的流程画出来像一颗倒置得树，因此得名“决策树”。&lt;/p&gt;

&lt;p&gt;决策树算法在分类领域应用广泛，并再此基础上衍生了随机森林等经典数据挖掘算法。包含策树的典型算法有ID3、C4.5、CART等。ID3和C4.5用于分类，CART可用于分类与回归。具体算法另有文章讨论，本文只讨论决策树的核心思想。&lt;/p&gt;

&lt;h3 id=&quot;决策树构建&quot;&gt;决策树构建&lt;/h3&gt;

&lt;p&gt;决策树算法的关键在于树（即分类规则）的构建，其核心步骤包括：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;节点（特征，也称属性）的选择及节点（特征）顺序。（第一步，先判断瓜蒂形状、瓜皮颜色还是敲瓜声音；第二步……）&lt;/li&gt;
  &lt;li&gt;节点如何划分。(根据瓜蒂形状如何进行先一步判断，例如瓜蒂卷，是好瓜，否则看瓜皮……)。&lt;/li&gt;
  &lt;li&gt;剪枝。去掉多余的叶结点（分类结果），使其回退到父结点（特征），甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;步骤1涉及到具体的算法，ID3，C4.5，CART各不相同：ID3使用信息增益，C4.5使用信息增益比，CART使用基尼指数。具体问题另有文章分别讨论。&lt;/p&gt;

&lt;p&gt;步骤2节点划分。节点的数据分为离散数据和连续数据，在处理离散数据时，可以将对应的属性值作为分支，或分组输出。在处理连续数据时，需要找到最佳划分点。例如一个特征的属性值范围是1-100，那么到底选择哪个点进行分类划分最为合理？&lt;/p&gt;

&lt;h3 id=&quot;最佳划分&quot;&gt;最佳划分&lt;/h3&gt;
&lt;p&gt;选择最佳划分的度量通常是根据划分后的子节点的纯度。纯度越高，说明划分后的分类越倾斜，也就越能达到分类的目的。纯度的度量一般可以用熵、基尼指数或分类误差。&lt;/p&gt;

&lt;p&gt;熵的值范围为0-1，基尼指数和分类误差值范围为0-0.5。&lt;/p&gt;

&lt;p&gt;步骤3可分为预剪枝和后剪枝，其目的是简化树，使其具有更好的泛化能力，避免过拟合。&lt;/p&gt;

&lt;h3 id=&quot;预剪枝&quot;&gt;预剪枝&lt;/h3&gt;
&lt;p&gt;预剪枝的思想是在构造决策树的同时进行剪枝，即在创建之前，先计算当前的划分是否能提升模型的泛化能力，如果不能的话，就不再创建分支，直接给出叶子节点（分类结果）。常见的预剪枝方法有：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;当决策树达到一定的深度的时候，停止分支；&lt;/li&gt;
  &lt;li&gt;当到达当前节点的样本数量小于某个阈值的时候，停止分支；&lt;/li&gt;
  &lt;li&gt;计算决策树每一次分裂对测试集的准确度是否提升，当没有提升或者提升程度小于某个阈值的时候，则停止分支；&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;后剪枝&quot;&gt;后剪枝&lt;/h3&gt;
&lt;p&gt;后剪枝是在决策树生长完成之后，对树进行剪枝，得到简化版的决策树。常见的后剪枝方法有：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;判断分支的熵的增加量是否小于某一阈值。如果是，则剪枝。&lt;/li&gt;
  &lt;li&gt;用测试集进行判断，如果某分支剪掉后的准确率有所提升或者没有降低，就剪枝。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;优点和局限&quot;&gt;优点和局限&lt;/h3&gt;

&lt;p&gt;优点：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;较随机森林可解释性，性能好。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;容易过拟合。&lt;/li&gt;
  &lt;li&gt;样本特征独立，忽略了特征之间的相关性。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;———本文结束&lt;/p&gt;

&lt;p&gt;［参考文档］&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cnblogs.com/luban/p/9412339.html&quot;&gt;https://www.cnblogs.com/luban/p/9412339.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/Oscar6280868/article/details/86158170&quot;&gt;https://blog.csdn.net/Oscar6280868/article/details/86158170&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>by 王勤晓</name></author><summary type="html">决策树（Decision Tree）是十大经典数据挖掘／机器学习算法，用于分类和回归。本文主要讨论分类。</summary></entry><entry><title type="html">关于自律</title><link href="http://localhost:4000/jekyll/update/2019/08/25/self-discipline.html" rel="alternate" type="text/html" title="关于自律" /><published>2019-08-25T23:00:00+08:00</published><updated>2019-08-25T23:00:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/08/25/self-discipline</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/08/25/self-discipline.html">&lt;p&gt;自律能帮助我们去掉不好的习惯，把精力更高效地投入到对自己有益的事情上。人人都知道自律是好的，但每个人自律的目的不一样。马云的自律和一位中学生自律的目的就不一样。&lt;/p&gt;

&lt;p&gt;近期发现，我自律的目的是为了获取安全感。自律学习是为了能保住工作，不至于失业；自律健身是为了健康，不至于猝死。&lt;/p&gt;

&lt;p&gt;根据马斯洛需求层次理论，人类需求像阶梯一样从低到高按层次分为五种，分别是：生理需求、安全需求、社交需求、尊重需求和自我实现需求。这样来看，我还处在第二层：我当前一切活动的主要目的还是为了实现安全需求，偶尔涉及更高层。&lt;/p&gt;

&lt;p&gt;对于芸芸大众来说，安全需求应该是最难实现的一层需求。财务自由是是实现安全需求最直接的方法，但赚钱是这个世上最难的事情之一（除了马云认为“赚钱是这个世上最简单的事”）。&lt;/p&gt;

&lt;p&gt;我的自律就是为了持续赚更多的钱，以满足我的安全需求。如此，才能有机会实现后面更高层的需求。&lt;/p&gt;

&lt;p&gt;———本文结束&lt;/p&gt;</content><author><name>by 王勤晓</name></author><summary type="html">自律能帮助我们去掉不好的习惯，把精力更高效地投入到对自己有益的事情上。人人都知道自律是好的，但每个人自律的目的不一样。马云的自律和一位中学生自律的目的就不一样。</summary></entry><entry><title type="html">数据挖掘算法－随机森林</title><link href="http://localhost:4000/jekyll/update/2019/08/25/RF.html" rel="alternate" type="text/html" title="数据挖掘算法－随机森林" /><published>2019-08-25T11:30:00+08:00</published><updated>2019-08-25T11:30:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/08/25/RF</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/08/25/RF.html">&lt;p&gt;随机森林（Random Forest，简称RF）是十大经典数据挖掘／机器学习算法，用于分类和回归。本文主要讨论分类。&lt;/p&gt;

&lt;h3 id=&quot;应用场景&quot;&gt;应用场景&lt;/h3&gt;

&lt;p&gt;了解一个算法，应该从其应用开始。&lt;/p&gt;

&lt;p&gt;RF算法用于将样本进行分类（通常二分类）。例如，笔者在帮助运营商识别潜在离网用户（目的是将用户分为将要离网OR不离网两类）中就使用了RF算法。&lt;/p&gt;

&lt;p&gt;RF算法的使用分为训练和推理两个阶段。&lt;/p&gt;

&lt;p&gt;训练阶段，输入训练数据集，即各种指标＋标签，如“用户类型，使用时长，网络质量等等 ＋ 是否离网标签“，经过RF算法训练，输出模型（发现的特征规律）。训练后的模型可反复使用。&lt;/p&gt;

&lt;p&gt;推理阶段，输入样本数据，经过训练阶段输出的RF模型识别，得到样本类别。&lt;/p&gt;

&lt;p&gt;RF算法在分类领域应用广泛。&lt;/p&gt;

&lt;h3 id=&quot;发展历史&quot;&gt;发展历史&lt;/h3&gt;

&lt;p&gt;随机森林算法是基于决策树算法改进而来。上世纪八十年代Breiman等人发明了决策树算法，通过反复二分数据进行分类，例如通过西瓜数据判断好瓜坏瓜，先看瓜皮颜色深还是浅，然后看瓜蒂是否卷曲，直到得到分类结果。决策树算法这里暂不深入讲解。&lt;/p&gt;

&lt;p&gt;2001年Breiman把多棵决策树组合成随机森林。随机森林顾名思义，该森林里面有很多的决策树，构造树的数据选择是随机的。进行分类时，输入样本进入森林，森林中的每一棵决策树对样本进行判断，看看这个样本应该属于哪一类，得票数最多的一类为最终分类结果。&lt;/p&gt;

&lt;p&gt;与决策树相比，随机森林在运算量没有显著提高的前提下提高了预测精度(其随机性解决过拟合问题)。&lt;/p&gt;

&lt;h3 id=&quot;工作原理&quot;&gt;工作原理&lt;/h3&gt;

&lt;p&gt;训练：RF使用多棵决策树，在构建每棵决策树时，对训练集进行随机且有放回地采样（bootstrap sample）。&lt;/p&gt;

&lt;p&gt;推理：RF训练出的模型包括多棵决策树，每棵决策树都是一个分类器，那么对于一个输入样本，N棵树会有N个分类结果。根据分类投票结果，将得票数最多的类别指定为最终输出，或者输出样本属于每个类别的概率。&lt;/p&gt;

&lt;p&gt;这种使用Boostrap采样方法，结合几个模型降低泛化误差的技术思想（避免过拟合），称为Bagging（Bootstrap aggregating）。&lt;/p&gt;

&lt;h3 id=&quot;优点和局限&quot;&gt;优点和局限&lt;/h3&gt;

&lt;p&gt;优点：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;在训练阶段，随机选择数据行和列，对于多特征、不平衡、特征遗失数据，效果良好。可不用做特征选择。&lt;/li&gt;
  &lt;li&gt;（较神经网络）运算量低，（较决策树）精度高。&lt;/li&gt;
  &lt;li&gt;可给出特征重要性（决策树也可以）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;随机森林已经被证明在某些噪音（存在着错误或异常(偏离期望值)的数据）较大的分类或者回归问题上会过拟合。&lt;/li&gt;
  &lt;li&gt;随机可能导致很多相似的决策树，影响真实结果。&lt;/li&gt;
  &lt;li&gt;不适合处理特征较少的数据，结果可能会不好。&lt;/li&gt;
  &lt;li&gt;比单棵决策树慢。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总之，随机森林是一种适合处理多维（多特征）数据、不平衡、特征遗失数据的优秀分类和回归算法。&lt;/p&gt;

&lt;p&gt;———本文结束&lt;/p&gt;

&lt;p&gt;［参考文档］&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/Sakura55/article/details/81413036#11-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%8E%86%E5%8F%B2&quot;&gt;https://blog.csdn.net/Sakura55/article/details/81413036#11-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%8E%86%E5%8F%B2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jianshu.com/p/d4b32cccd747&quot;&gt;https://www.jianshu.com/p/d4b32cccd747&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>by 王勤晓</name></author><summary type="html">随机森林（Random Forest，简称RF）是十大经典数据挖掘／机器学习算法，用于分类和回归。本文主要讨论分类。</summary></entry><entry><title type="html">MVX 模式</title><link href="http://localhost:4000/jekyll/update/2019/08/17/MVX.html" rel="alternate" type="text/html" title="MVX 模式" /><published>2019-08-17T19:30:00+08:00</published><updated>2019-08-17T19:30:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/08/17/MVX</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/08/17/MVX.html">&lt;p&gt;软件开发工作者经常会听到MVC，MVP，MVVM框架。这些MVX框架，不是具体的工程技术，而是一种软件设计的模式，将软件项目的开发从逻辑上分层，体现了软件工程中模块化和解藕的思想。&lt;/p&gt;

&lt;p&gt;众多MVX框架，都源于MVC，或者说是MVC的变种。&lt;/p&gt;

&lt;h3 id=&quot;mvc&quot;&gt;MVC&lt;/h3&gt;

&lt;p&gt;MVC（Model-View-Controller, 模型-视图-控制器）。MVC模式最早出现在Java领域，随后衍生到前端开发等领域。&lt;/p&gt;

&lt;p&gt;MVC模式认为一个软件项目的代码在逻辑组织上应该分为三层，分别是模型层（M），视图层（V），控制器层（C）。模型层提供数据输入，视图层展示输出结果，控制器层进行逻辑处理。三层彼此独立，依靠接口通信。这种设计模式，避免不同逻辑层耦合，某一层的修改不涉及其他层的修改，其修改对其他层也不可见。这种设计让代码开发和维护起来更简单。&lt;/p&gt;

&lt;p&gt;MVC模式中各层的通信是单向的。一般是V或者用户发出请求给＝＝》C，然后C通知＝＝》M要准备哪些数据，M准备数据并发送给＝＝》V。&lt;/p&gt;

&lt;h3 id=&quot;mvp&quot;&gt;MVP&lt;/h3&gt;

&lt;p&gt;MVP（Model-View-Presenter, 模型-视图-呈现器）。MVP在MVC的基础上改进，与MVC的不同是1）Contrller变成了Presenter，2）各层的通信变成了双向，3）V与M不再直接联系，所有通信经过P处理。减轻了View层的工作，对应增加了Presenter层的工作。&lt;/p&gt;

&lt;h3 id=&quot;mvvm&quot;&gt;MVVM&lt;/h3&gt;

&lt;p&gt;MVVM（Model-View-ViewModel, 模型-视图-视图模型）。MVVM进一步在MVP的基础上改进，与MVP的不同是1）Presenter变成了ViewModel，2）V与VM双向绑定，一方的变动会自动同步到另一方。&lt;/p&gt;

&lt;p&gt;现在的主流前端框架如Angualr就是采用MVVM模式。MVVM的设计思想：关注Js对象（Model）的变化，让MVVM框架去自动更新DOM（View）的状态，从而把开发者从操作DOM的繁琐步骤中解脱出来。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;我的理解：MVC，MVP还是MVVM的差异主要实在各层的分工上，无非是V层厚一点，或者C层工作职责多一点，具体的场景选什么看需求。但MVX核心思想没有变，那就是模块化和解藕，这是在任何软件项目开发中都应该遵循的原则。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;———本文结束&lt;/p&gt;

&lt;p&gt;［参考文档］&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ruanyifeng.com/blog/2015/02/mvcmvp_mvvm.html&quot;&gt;http://www.ruanyifeng.com/blog/2015/02/mvcmvp_mvvm.html&lt;/a&gt;
&lt;a href=&quot;https://www.liaoxuefeng.com/wiki/1022910821149312/1108898947791072&quot;&gt;https://www.liaoxuefeng.com/wiki/1022910821149312/1108898947791072&lt;/a&gt;&lt;/p&gt;</content><author><name>by 王勤晓</name></author><summary type="html">软件开发工作者经常会听到MVC，MVP，MVVM框架。这些MVX框架，不是具体的工程技术，而是一种软件设计的模式，将软件项目的开发从逻辑上分层，体现了软件工程中模块化和解藕的思想。</summary></entry><entry><title type="html">《黑客与画家》句抄</title><link href="http://localhost:4000/jekyll/update/2019/08/10/hackers_and_painters_notes.html" rel="alternate" type="text/html" title="《黑客与画家》句抄" /><published>2019-08-10T16:30:00+08:00</published><updated>2019-08-10T16:30:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/08/10/hackers_and_painters_notes</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/08/10/hackers_and_painters_notes.html">&lt;p&gt;本周没有新的阅读。接&lt;a href=&quot;https://wangqinxiao.github.io/jekyll/update/2019/07/28/why_nerds_are_unpopular_notes.html&quot;&gt;上篇&lt;/a&gt;继续整理下Hackers and Painters（黑客与画家）一书的句子。&lt;/p&gt;

&lt;p&gt;《黑客与画家》是该书中的一篇文章。本文不讨论该文思想，只记录几个有所启发的句子。&lt;/p&gt;

&lt;p&gt;1.&lt;code class=&quot;highlighter-rouge&quot;&gt;The only external test is time. Over time, beautiful things tend to thrive, and ugly things tend to get discarded. Unfortunately, the amounts of time involved can be longer than human lifetimes.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;任何事物的评判都需要一个标准，学术圈靠论文、职场靠KPI、相亲靠高富帅。作者认为黑客的工作更像艺术，难以有客观标准，唯有时间能检验出真理／美，但问题是时间往往会很长。时间的筛选虽然漫长，但无疑都是经典的普世文化。每个人都逃不过时间的宿命，如果能留下点什么有价值的痕迹，不枉此生；如果不能，在有限的时间里阅读时间长河留下的礼物，也是一件美事。&lt;/p&gt;

&lt;p&gt;2.&lt;code class=&quot;highlighter-rouge&quot;&gt;Everyone in the sciences secretly believes that mathematicians are smarter than they are. I think mathematicians also believe this. At any rate, the result is that scientists tend to make their work look as mathematical as possible.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;人们都认为数学家是最聪明的群体。所以科学家们都喜欢把自己的工作变得像数学研究，最直接的体现就是在文章中使用公式和希腊字母。把问题数学抽象是一种归纳的方法，但不少投机之徒利用人们对数学的敬畏，故意制造障碍，以浑水摸鱼或装逼。此现象不可避免，唯一的方法就是自己掌握数学，才能识别出南郭先生。另一方面，如果真想让受众明白自己的表述，就能少用公式就少用，正如霍金曾经说过，“你多写一个公式，就会少一半的读者”。&lt;/p&gt;

&lt;p&gt;3.&lt;code class=&quot;highlighter-rouge&quot;&gt;Empathy is probably the single most important difference between a good hacker and a great one. Some hackers are quite smart, but when it comes to empathy are practically solipsists. It's hard for such people to design great software, because they can't see things from the user's point of view.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;好的黑客和优秀黑客的区别在于是否拥有“同理心”，站在用户的角度思考，才能做出优秀的产品。演讲、写作、处理人际关系、谈恋爱亦如此。&lt;/p&gt;

&lt;p&gt;4.&lt;code class=&quot;highlighter-rouge&quot;&gt;Programs should be written for people to read, and only incidentally for machines to execute.You need to have empathy not just for your users, but for your readers. It's in your interest, because you'll be one of them. Many a hacker has written a program only to find on returning to it six months later that he has no idea how it works. I know several people who've sworn off Perl after such experiences.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;程序是写给人看的，顺便让机器执行。作者认为开发者不仅要对用户有同理心，对程序的读者也要有同理心，这对开发者自己也有利，因为开发者自己也是读者的一份子。开发者如果便写的程序可读性差，可能自己几个月回来也看不懂了，更别说让其他人维护。正如乔布斯对待iPhone，不仅外表设计要完美，内部零件结构也要精益求精，正是这种态度才能造就经得起时间考验的艺术品。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.paulgraham.com/hp.html&quot;&gt;黑客与画家 原文地址&lt;/a&gt;&lt;/p&gt;</content><author><name>by 王勤晓</name></author><summary type="html">本周没有新的阅读。接上篇继续整理下Hackers and Painters（黑客与画家）一书的句子。</summary></entry><entry><title type="html">Spark程序运行过程</title><link href="http://localhost:4000/jekyll/update/2019/08/10/SparkExecuting.html" rel="alternate" type="text/html" title="Spark程序运行过程" /><published>2019-08-10T16:30:00+08:00</published><updated>2019-08-10T16:30:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/08/10/SparkExecuting</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/08/10/SparkExecuting.html">&lt;p&gt;笔者在一线碰到过很多次Spark程序崩溃的问题，而且解决起来比较费劲。纠其根本原因在于开发人员在编写Spark程序时只注重功能实现，不了解Spark程序运行过程，程序性能不佳，导致现场运行问题频发。而解决问题的过程中，由于不了解运行过程，只能靠各种尝试，解决问题效率极低。&lt;/p&gt;

&lt;p&gt;总之，不了解Spark运行原理，就没法写出可靠的Spark程序。&lt;/p&gt;

&lt;p&gt;现在就用一段Spark代码实例来一步一步解析Spark运行过程。&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# Spark program&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;textFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputPath1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;···&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;···&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;textFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputPath2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;···&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;textFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputPath3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtinone1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;union&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lines3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtinone&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtinone1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dtinone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;saveAsTextFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;···&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dtinone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;···&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;foreach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;···&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;程序运行过程如下：&lt;/p&gt;

&lt;h1 id=&quot;1-资源配置&quot;&gt;1. 资源配置&lt;/h1&gt;
&lt;p&gt;资源分配不由Spark程序决定，而是由资源管理管理器决定，如Hadoop Yarn。大数据计算需要大量硬件计算资源，资源分配决定着程序的运行性能。运行Spark程序前需要保证已配置足够可用的计算资源。资源配置参数包括，资源队列、Driver内存、Executor内存、Executor核数等等。&lt;/p&gt;

&lt;p&gt;资源配置是Spark程序运行前的准备工作，本文暂且不做深入探讨。&lt;/p&gt;

&lt;h1 id=&quot;2-创建sparkcontext&quot;&gt;2. 创建SparkContext&lt;/h1&gt;
&lt;p&gt;运行Spark程序后，第一步Driver进程启动并创建SparkContext，即该Spark程序的专属运行环境。SparkContext负责资源的申请，任务的分配和管理。相当于Spark程序运行过程中的管理者。以下步骤，除了计算，其他几乎都有SparkContext来负责。&lt;/p&gt;

&lt;p&gt;当程序运行完成后，Driver会关闭SparkContext。&lt;/p&gt;

&lt;h1 id=&quot;3-启动executor&quot;&gt;3. 启动Executor&lt;/h1&gt;
&lt;p&gt;SparkContext再创建完成后，会向资源管理器申请Executor资源，并启动Executor。&lt;/p&gt;

&lt;h1 id=&quot;4-创建dag图&quot;&gt;4. 创建DAG图&lt;/h1&gt;
&lt;p&gt;环境和资源一切就绪之后。SparkContext会根据Spark程序创建DAG（Directed Acycle graph，有向无环图），即反应所有RDD之间依赖关系的图。&lt;/p&gt;

&lt;p&gt;本文示例的程序DAG图大致如下：（本图比较简易，默认RDD没有分区。基本元素单位应该是是RDD的分区）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/DAG.jpg&quot; alt=&quot;DAG&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-触发job&quot;&gt;3. 触发Job&lt;/h1&gt;
&lt;p&gt;根据DAG图，一次RDD的Action触发一次Job（计算作业）。例如，示例代码中的saveAsTextFile就会触发一次Job。该Job包括对应RDD上的各种操作。&lt;/p&gt;

&lt;h1 id=&quot;4-分配stagetaskset&quot;&gt;4. 分配Stage（TaskSet）&lt;/h1&gt;
&lt;p&gt;Job可进一步分为Stage。分配原则为根据DAG图，Job流程从后向前，遇到宽依赖，则将当前的流程分为一个Stage。各个Stage一般并非只有线性关系，还有嵌套、并行关系。&lt;/p&gt;

&lt;p&gt;本文代码例子的Stag划分如图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/JOB.jpg&quot; alt=&quot;JOB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一个Stage对应一个TaskSet，即包含多个Task的集合。每个Task对于一个RDD（分区）的操作。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/TASK.jpg&quot; alt=&quot;TASK&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;6-提交stagetaskset&quot;&gt;6. 提交Stage（TaskSet）&lt;/h1&gt;
&lt;p&gt;提交Stage，也就是提交TaskSet。DAGScheduler通过TaskScheduler接口提交TaskSet给各个Executor，并以多线程的形式执行。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/TASK-SET.jpg&quot; alt=&quot;TASK-SET&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;7-执行task&quot;&gt;7. 执行Task&lt;/h1&gt;
&lt;p&gt;TaskScheduler构建一个TaskSetManager的实例来管理一个TaskSet的生命周期，跟踪每一个task，如果task失败，负责重试task直到达到task重试次数的最多次数。&lt;/p&gt;

&lt;p&gt;一个TaskSet在Executor中执行结束后，其结果会返回给DAGScheduler。如果得到TaskSet执行失败的信息，则会重新动态分配该Task到其他节点执行，直到重试次数的最多次数（根据笔者经验，应该是默认4次，如果继续失败，则程序崩溃）。&lt;/p&gt;

&lt;p&gt;[参考资料]：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/liuxiangke0210/article/details/79687240&quot;&gt;https://blog.csdn.net/liuxiangke0210/article/details/79687240&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>by 王勤晓</name></author><summary type="html">笔者在一线碰到过很多次Spark程序崩溃的问题，而且解决起来比较费劲。纠其根本原因在于开发人员在编写Spark程序时只注重功能实现，不了解Spark程序运行过程，程序性能不佳，导致现场运行问题频发。而解决问题的过程中，由于不了解运行过程，只能靠各种尝试，解决问题效率极低。</summary></entry><entry><title type="html">大数据处理二剑客之Spark</title><link href="http://localhost:4000/jekyll/update/2019/08/04/Hadoop&Spark1.html" rel="alternate" type="text/html" title="大数据处理二剑客之Spark" /><published>2019-08-04T17:25:00+08:00</published><updated>2019-08-04T17:25:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/08/04/Hadoop&amp;Spark1</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/08/04/Hadoop&amp;Spark1.html">&lt;p&gt;进入大数据领域，Hadoop和Spark是最先要了解的两个技术，至少目前看来是这样的。&lt;/p&gt;

&lt;p&gt;虽然一直在用它们，但对其没有系统的认识，甚至把二者混为一谈。现在梳理一下。&lt;/p&gt;

&lt;p&gt;要了解一项技术，首先思考它是干什么用的？&lt;code class=&quot;highlighter-rouge&quot;&gt;大数据领域的工作包括&quot;数据处理&quot;和&quot;数据分析&quot;&lt;/code&gt;。数据处理似食材准备，数据分析似烹饪过程。准备食材包括买菜、洗菜、切菜、腌制等过程，为下一步烹饪做准备；数据处理包括数据收集、存储、清洗、转换、组合等动作，方便我们进行下一步数据分析。&lt;code class=&quot;highlighter-rouge&quot;&gt;Hadoop和Spark就是数据处理这一阶段（大数据分析也有涉及，但以处理为主）的关键两项技术，二者有重合，但各司其职.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这篇文章梳理Spark。首先，&lt;code class=&quot;highlighter-rouge&quot;&gt;一句话描述Spark：最受欢迎的大数据计算框架&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;Spark是加州大学伯克利分校的AMP实验室开发的类似Hadoop MapReduce的通用并行框架，诞生于2009年，2013年成为了Aparch基金项目。Spark是MapReduce的替代方案，而且兼容HDFS、Hive，可融入Hadoop的生态系统，以弥补MapReduce的不足。&lt;/p&gt;

&lt;h1 id=&quot;spark-vs-hadoop&quot;&gt;Spark VS Hadoop&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://wangqinxiao.github.io/jekyll/update/2019/07/28/Hadoop&amp;amp;Spark.html&quot;&gt;大数据处理二剑客之Hadoop&lt;/a&gt;一文梳理了Hadoop生态系统，Hadoop已有MapReduce计算框架，为什么又要用Spark作为大数据计算框架呢？&lt;/p&gt;

&lt;p&gt;答案是：因为&lt;code class=&quot;highlighter-rouge&quot;&gt;Spark比Hadoop MapReduce更快&lt;/code&gt;。Spark比MapReduce更快的原因是，Hadoop MapReduce直接读写存储设备硬盘（HDFS），而&lt;code class=&quot;highlighter-rouge&quot;&gt;Spark基于内存进行计算&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;Spark和Hadoop MaoReduce计算框架一样，是分布式架构。其特点是在数据计算的过程中，把中间结果缓存在内存中。在进行大量数据计算时，直接从内存中读取数据，这要比从硬盘中读取快很多，速度优势明显。&lt;/p&gt;

&lt;p&gt;但是，基于内存的计算同样也会带来缺点。&lt;code class=&quot;highlighter-rouge&quot;&gt;与Hadoop MapReduce相比，Spark的缺点是不稳定&lt;/code&gt;。内存毕竟有限，成本也高，如果数据量过大的话，容易造成内存溢出的问题，从而导致计算过程崩溃。Hadoop MapReduce写的程序虽然慢，但是总会算出结果。而Spark写的程序常常由于数据量过大、内存不够或者计算资源配置不合理，导致崩溃（如Lost stages等等）。笔者在一线使用Spark进行数据处理，程序崩溃是最头疼的事情。&lt;/p&gt;

&lt;p&gt;鉴于Spark基于内存计算而导致的速度快的优点和不稳定的缺点。在大数据项目的计算框架技术选型时，需要&lt;code class=&quot;highlighter-rouge&quot;&gt;综合考虑数据量、业务的时间要求、可用计算资源&lt;/code&gt;。一般在&lt;code class=&quot;highlighter-rouge&quot;&gt;数据处理阶段（原始数据量一般较大）的过程中用Hadoop MapReduce&lt;/code&gt;，在&lt;code class=&quot;highlighter-rouge&quot;&gt;数据分析（数据已经过清洗，数据量一般较小）的过程中使用Spark&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;Spark已经融入Hadoop系统，可支持Hadoop Yarn资源管理，HDFS进行数据存储。&lt;/p&gt;

&lt;h1 id=&quot;spark组成部分&quot;&gt;Spark组成部分&lt;/h1&gt;

&lt;p&gt;Spark的主要组件有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SparkCore：Spark核心计算组件，实现分布式计算。它是我们最常用到的组件。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;＊ SparkSQL：Spark Sql 是Spark来操作结构化数据的程序包，可以让我使用Hive SQL语句的方式来查询和处理数据。&lt;/p&gt;

&lt;p&gt;SparkStreaming： 是Spark提供的对实时数据进行流式计算的组件。&lt;/p&gt;

&lt;p&gt;MLlib：提供常用机器学习算法的实现库。&lt;/p&gt;

&lt;p&gt;GraphX：提供一个分布式图计算框架，能高效进行图计算。&lt;/p&gt;

&lt;h1 id=&quot;spark相关概念&quot;&gt;Spark相关概念&lt;/h1&gt;

&lt;p&gt;这部分梳理我们最常用到的Spark Core的运行原理。Spark程序在运行过程中，涉及到的概念包括Application、Driver、Executer、Job、Stage、RDD等。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Application：编写的Spark应用程序就是一个Application。&lt;/li&gt;
  &lt;li&gt;Driver：Driver运行Application的Main()函数并创建SparkContext，即应用程序的运行环境。SparkContext负责分布式Cluster间的通信、任务分配管理等。通常SparkContext即代表Driver。当Executor部分运行完毕后，Driver负责将SparkContext关闭&lt;/li&gt;
  &lt;li&gt;Excuter：执行器。一个Excuter代表一个进程，负责计算任务，并将结果存在内存或者磁盘上。Excuter越多，说明进程越多，执行速度也就更快。&lt;/li&gt;
  &lt;li&gt;Job：Job是一个计算作业，由一个或着多个任务集组成。一次Spark Action，例如ReduceByKey就会催生一次计算作业，行成一个Job。一个Job包含多个RDD及作用于相应RDD上的各种Operation。&lt;/li&gt;
  &lt;li&gt;Stage：Stage是一个任务集对应的调度阶段。每个Job会被拆分很多组Task，每组任务被称为Stage（或者TaskSet）。&lt;/li&gt;
  &lt;li&gt;Task：任务被送到某个Executor上的工作任务;单个分区数据集上的最小处理流程单元.&lt;/li&gt;
  &lt;li&gt;RDD：弹性分布式数据集（Resilient Distributed Datasets，RDD），Spark的一种数据对象，是Spark的基本计算单元。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[参考资料]：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cnblogs.com/xia520pi/p/8609938.html&quot;&gt;https://www.cnblogs.com/xia520pi/p/8609938.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://baike.baidu.com/item/SPARK/2229312?fr=aladdin&quot;&gt;https://baike.baidu.com/item/SPARK/2229312?fr=aladdin&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/liuxiangke0210/article/details/79687240&quot;&gt;https://blog.csdn.net/liuxiangke0210/article/details/79687240&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>by 王勤晓</name></author><summary type="html">进入大数据领域，Hadoop和Spark是最先要了解的两个技术，至少目前看来是这样的。</summary></entry><entry><title type="html">《为什么书呆子不受欢迎》句抄</title><link href="http://localhost:4000/jekyll/update/2019/07/28/why_nerds_are_unpopular_notes.html" rel="alternate" type="text/html" title="《为什么书呆子不受欢迎》句抄" /><published>2019-07-28T19:54:00+08:00</published><updated>2019-07-28T19:54:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/07/28/why_nerds_are_unpopular_notes</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/07/28/why_nerds_are_unpopular_notes.html">&lt;p&gt;最近在看硅谷创业教父Paul Graham（保罗·格雷厄姆）的Hackers and Painters（黑客与画家），一本收录其博客的文集，讲述其对计算机文化的思考。&lt;/p&gt;

&lt;p&gt;断断续续，每周末抽出时间来读，只读了三分之一，没有记笔记，也不求甚解。回想这种读书方法是不正确的，这本书虽不是传世经典，需熟读百遍才能其义自见，但只囫囵吞枣的读一遍还是不够的。起码有个读书笔记，加上自己的思考，想想如何学以致用，再不济也要抄几个会心的句子。&lt;/p&gt;

&lt;p&gt;此前读的几篇，自然是没有笔记，但遇到不错的句子还是随手划了记号。今日先总结一篇，权当好好读书的开始。&lt;/p&gt;

&lt;h3 id=&quot;why-nerds-are-unpopuler-为什么书呆子不受欢迎&quot;&gt;Why Nerds Are Unpopuler 为什么书呆子不受欢迎&lt;/h3&gt;

&lt;p&gt;本文讲述为什么Nerds（搞技术的书呆子）不受欢迎？从Nerds本身、人类社会学、家长、学校等方面都做了分析，最后把教育制度狠狠的批判了一番。&lt;/p&gt;

&lt;p&gt;1.&lt;code class=&quot;highlighter-rouge&quot;&gt;Another reason kids persecute nerds is to make themselves feel better. When you tread water, you lift yourself up by pushing water down. Likewise, in any social hierarchy, people unsure of their own position will try to emphasize it by maltreating those they think rank below. I've read that this is why poor whites in the United States are the group most hostile to blacks.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;其他孩子欺负Nerds的一个原因是通过打压别人，抬高自己，从而从心理上获得平衡。在人类社会中，往往对自己的地位不够自信的人才会欺凌比自己阶层低的人，底层人民狗咬狗，大狗咬小狗。比如，在美国最敌视黑人的是穷的白人。对于个人而言，避免自己称为被欺凌的对象，或者靠欺凌获得心理安慰的“穷白人”，只能是往更高的阶层爬升。&lt;/p&gt;

&lt;p&gt;2.&lt;code class=&quot;highlighter-rouge&quot;&gt;Like a politician who wants to distract voters from bad times at home, you can create an enemy if there isn't a real one. By singling out and persecuting a nerd, a group of kids from higher in the hierarchy create bonds between themselves. Attacking an outsider makes them all insiders. This is why the worst cases of bullying happen with groups.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;小孩子们欺负Nerds的另一个原因是为了合群。形成一个团体最有效的办法是找到一个共同的敌人，对外同仇敌忾，内部才显得团结。政客往往会用这一招来获得国内民众的支持，最近的特朗普在2020大选之前打压华为，少不了这方面的考虑。这招够损，但实用。&lt;/p&gt;

&lt;p&gt;3.&lt;code class=&quot;highlighter-rouge&quot;&gt;Bullying was only part of the problem. Another problem, and possibly an even worse one, was that we never had anything real to work on. Humans like to work; in most of the world, your work is your identity. And all the work we did was pointless, or seemed so at the time.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;校园霸凌还有一个原因，就是闲的没事干，吃饱了撑着找找茬。作者对现在教育制度大家批判，认为都是家长和学校太懒，学生在学校做的事情没有意义。作者观点似乎太偏激，只提出了问题，没给出有效的解决方案，这里不多加讨论。我记下这句话的原因是因为他提到：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;人其实都是喜欢工作的，在大多数情况下，工作就是个人的身份。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果我们讨厌工作，觉得工作很苦很累，往往不是因为工作强度大、时间长，而是因为觉得工作没有意义，没有收获感。对于成年人来说，工作是生活的主题，并影响甚至决定家庭、感情等其他各个方面。因此，对的职业规划显得更加重要。&lt;/p&gt;

&lt;p&gt;本文完。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.paulgraham.com/nerds.html&quot;&gt;hy Nerds Are Unpopuler 原文地址&lt;/a&gt;&lt;/p&gt;</content><author><name>by 王勤晓</name></author><summary type="html">最近在看硅谷创业教父Paul Graham（保罗·格雷厄姆）的Hackers and Painters（黑客与画家），一本收录其博客的文集，讲述其对计算机文化的思考。</summary></entry><entry><title type="html">大数据处理二剑客之Hadoop</title><link href="http://localhost:4000/jekyll/update/2019/07/28/Hadoop&Spark.html" rel="alternate" type="text/html" title="大数据处理二剑客之Hadoop" /><published>2019-07-28T15:44:00+08:00</published><updated>2019-07-28T15:44:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/07/28/Hadoop&amp;Spark</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/07/28/Hadoop&amp;Spark.html">&lt;p&gt;进入大数据领域，Hadoop和Spark是最先要了解的两个技术，至少目前看来是这样的。&lt;/p&gt;

&lt;p&gt;虽然一直在用它们，但对其没有系统的认识，甚至把二者混为一谈。现在梳理一下。&lt;/p&gt;

&lt;p&gt;要了解一项技术，首先思考它是干什么用的？&lt;code class=&quot;highlighter-rouge&quot;&gt;大数据领域的工作包括&quot;数据处理&quot;和&quot;数据分析&quot;&lt;/code&gt;。数据处理似食材准备，数据分析似烹饪过程。准备食材包括买菜、洗菜、切菜、腌制等过程，为下一步烹饪做准备；数据处理包括数据收集、存储、清洗、转换、组合等动作，方便我们进行下一步数据分析。&lt;code class=&quot;highlighter-rouge&quot;&gt;Hadoop和Spark就是数据处理这一阶段（大数据分析也有涉及，但以处理为主）的关键两项技术，二者有重合，但各司其职.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这篇文章先梳理Hadoop。首先，&lt;code class=&quot;highlighter-rouge&quot;&gt;一句话描述Hadoop：大数据分析处理领域的分布式系统基础架构&lt;/code&gt;。Hadoop核心思想是分布式，即将任务分到多台计算机上进行处理。&lt;/p&gt;

&lt;h1 id=&quot;起源受启发于google&quot;&gt;起源：受启发于Google&lt;/h1&gt;

&lt;p&gt;Hadoop的诞生比Spark要早。可追溯到2004年，两个程序员Doug Cutting和Mike Cafarella受Google Lab 开发的Map/Reduce和Google File System(GFS)的启发，开始实施最初的版本（称为HDFS和MapReduce），最初Hadoop只与网页索引有关，用于处理海量网页数据进行搜索。随后由Apache基金会支持开发，逐渐发展为一个大数据分析处理领域的分布式系统基础架构。&lt;/p&gt;

&lt;p&gt;该项目的创建者，Doug Cutting解释Hadoop的得名 ：“这个名字是我孩子给一个棕黄色的大象玩具命名的“.&lt;/p&gt;

&lt;p&gt;Hadoop的主要组成部分是HDFS和MamReduce。&lt;/p&gt;

&lt;h1 id=&quot;hdfs分布式文件系统&quot;&gt;HDFS：分布式文件系统&lt;/h1&gt;
&lt;p&gt;HDFS（Hadoop Distributed File System，Hadoop分布式文件系统，顾名思义是用于存储数据的系统，与传统的分级文件系统（使用目录组织文件）相比，其一样可以增、删、改、查，HDFS的不同之处是分布式存储。&lt;/p&gt;

&lt;p&gt;分布式存储指的是存储在HDFS中的文件被分成块。HDFS有两个关键概念：NameNode和DataNode，NameNode可以控制所有文件的操作，DataNode用于存储文件。分布式文件系统设计的优势在于可支持海量数据de的快速存储和查询等操作。&lt;/p&gt;

&lt;h1 id=&quot;mapreduce分布式计算框架&quot;&gt;MapReduce：分布式计算框架&lt;/h1&gt;
&lt;p&gt;MapReduce，顾名思义其核心是Map（影射）函数和Reduce（化简、规约）函数。&lt;/p&gt;

&lt;p&gt;Map函数遍历集合里的每个目标对其应用同一个操作。再用烹饪的例子，在准备食材时需要洗菜，把茄子、辣椒、黄瓜……都洗一遍，逐个清洗这一过程就是Map，集合对象是各种食材，同一操作就是清洗。&lt;/p&gt;

&lt;p&gt;Reduce函数遍历集合里的每个目标将其综合称为一个结果。还是烹饪的例子，菜洗好后，开始做大杂烩，锅里加入茄子、加入辣椒、加入黄瓜……最终得到一盆大杂烩。加入各种食材这一过程就是Reduce，将多种食材归约为最重的一个结果———大杂烩菜。&lt;/p&gt;

&lt;p&gt;MapReduce这两个核心函数并不新鲜，其内容不仅是这二个函数（其他如Group，Sort等），而是一种编程模型，是一种方法，抽象理论。MapReduce借鉴了语言Lisp的函数式编程设计（什么是函数式编程，新文再议），体现了大数据处理过程中分而治之的思想。分布式是分而治之思想的实践，MapReduce专为分布式计算设计。&lt;/p&gt;

&lt;p&gt;除了HDFS和MapReduce，Hadoop还包括以下模块：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Hadoop Common：支持其他Hadoop模块的通用工具。&lt;/li&gt;
  &lt;li&gt;Hadoop YARN：一种作业调度和集群资源管理的框架。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Apache中其他Hadoop相关的项目包括：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Ambari：一种用于提供、管理和监督Apache Hadoop集群的基于Web UI的且易于使用的Hadoop管理工具。&lt;/li&gt;
  &lt;li&gt;Avro：一种数据序列化系统。&lt;/li&gt;
  &lt;li&gt;Cassandra：一种无单点故障的可扩展的分布式数据库。&lt;/li&gt;
  &lt;li&gt;Chukwa：一种用于管理大型分布式系统的数据收集系统。&lt;/li&gt;
  &lt;li&gt;HBase：一种支持存储大型表的结构化存储的可扩展的分布式数据库。&lt;/li&gt;
  &lt;li&gt;Hive：一种提供数据汇总和特定查询的数据仓库。&lt;/li&gt;
  &lt;li&gt;Mahout：一种可扩展的机器学习和数据挖掘库（Scala语言实现，可结合Spark后端）。&lt;/li&gt;
  &lt;li&gt;Pig：一种高级的数据流语言且支持并行计算的执行框架（2017年发布的最新版本0.17.0是添加了Spark上的Pig应* 用）。&lt;/li&gt;
  &lt;li&gt;Spark：一种用于Hadoop数据的快速通用计算引擎。Spark提供一种支持广泛应用的简单而易懂的编程模型，包括* ETL（ Extract-Transform-Load）、机器学习、流处理以及图计算。&lt;/li&gt;
  &lt;li&gt;Tez：一种建立在Hadoop YARN上数据流编程框架，它提供了一个强大而灵活的引擎来任意构建DAG* （Directed-acyclic-graph）任务去处理用于批处理和交互用例的数据。&lt;/li&gt;
  &lt;li&gt;ZooKeeper：一种给分布式应用提供高性能的协同服务系统。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;那么问题，既然Hadoop生态家族这么庞大，我们为什么要选择Spark作为对于大数据进行数据分析和数据挖掘的基本计算框架？欲知后事如何，请听下回分解。&lt;/p&gt;

&lt;p&gt;[参考资料]：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.51cto.com/xpleaf/2080181&quot;&gt;https://blog.51cto.com/xpleaf/2080181&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://baike.baidu.com/item/Hadoop/3526507&quot;&gt;https://baike.baidu.com/item/Hadoop/3526507&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>by 王勤晓</name></author><summary type="html">进入大数据领域，Hadoop和Spark是最先要了解的两个技术，至少目前看来是这样的。</summary></entry></feed>
