<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="http://localhost:4000/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.4.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-08-18T11:00:51+08:00</updated><id>http://localhost:4000//</id><title type="html">Hi, I’m Wang qinxiao</title><subtitle>读有趣的书，做有趣的事。
</subtitle><author><name>by 王勤晓</name></author><entry><title type="html">MVX 模式</title><link href="http://localhost:4000/jekyll/update/2019/08/17/MVX.html" rel="alternate" type="text/html" title="MVX 模式" /><published>2019-08-17T19:30:00+08:00</published><updated>2019-08-17T19:30:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/08/17/MVX</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/08/17/MVX.html">&lt;p&gt;软件开发工作者经常会听到MVC，MVP，MVVM框架。这些MVX框架，不是具体的工程技术，而是一种软件设计的模式，将软件项目的开发从逻辑上分层，体现了软件工程中模块化和解藕的思想。&lt;/p&gt;

&lt;p&gt;众多MVX框架，都源于MVC，或者说是MVC的变种。&lt;/p&gt;

&lt;h3 id=&quot;mvc&quot;&gt;MVC&lt;/h3&gt;

&lt;p&gt;MVC（Model-View-Controller, 模型-视图-控制器）。MVC模式最早出现在Java领域，随后衍生到前端开发等领域。&lt;/p&gt;

&lt;p&gt;MVC模式认为一个软件项目的代码在逻辑组织上应该分为三层，分别是模型层（M），视图层（V），控制器层（C）。模型层提供数据输入，视图层展示输出结果，控制器层进行逻辑处理。三层彼此独立，依靠接口通信。这种设计模式，避免不同逻辑层耦合，某一层的修改不涉及其他层的修改，其修改对其他层也不可见。这种设计让代码开发和维护起来更简单。&lt;/p&gt;

&lt;p&gt;MVC模式中各层的通信是单向的。一般是V或者用户发出请求给＝＝》C，然后C通知＝＝》M要准备哪些数据，M准备数据并发送给＝＝》V。&lt;/p&gt;

&lt;h3 id=&quot;mvp&quot;&gt;MVP&lt;/h3&gt;

&lt;p&gt;MVP（Model-View-Presenter, 模型-视图-呈现器）。MVP在MVC的基础上改进，与MVC的不同是1）Contrller变成了Presenter，2）各层的通信变成了双向，3）V与M不再直接联系，所有通信经过P处理。减轻了View层的工作，对应增加了Presenter层的工作。&lt;/p&gt;

&lt;h3 id=&quot;mvvm&quot;&gt;MVVM&lt;/h3&gt;

&lt;p&gt;MVVM（Model-View-ViewModel, 模型-视图-视图模型）。MVVM进一步在MVP的基础上改进，与MVP的不同是1）Presenter变成了ViewModel，2）V与VM双向绑定，一方的变动会自动同步到另一方。&lt;/p&gt;

&lt;p&gt;现在的主流前端框架如Angualr就是采用MVVM模式。MVVM的设计思想：关注Js对象（Model）的变化，让MVVM框架去自动更新DOM（View）的状态，从而把开发者从操作DOM的繁琐步骤中解脱出来。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;我的理解：MVC，MVP还是MVVM的差异主要实在各层的分工上，无非是V层厚一点，或者C层工作职责多一点，具体的场景选什么看需求。但MVX核心思想没有变，那就是模块化和解藕，这是在任何软件项目开发中都应该遵循的原则。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;———本文结束&lt;/p&gt;

&lt;p&gt;［参考文档］&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ruanyifeng.com/blog/2015/02/mvcmvp_mvvm.html&quot;&gt;http://www.ruanyifeng.com/blog/2015/02/mvcmvp_mvvm.html&lt;/a&gt;
&lt;a href=&quot;https://www.liaoxuefeng.com/wiki/1022910821149312/1108898947791072&quot;&gt;https://www.liaoxuefeng.com/wiki/1022910821149312/1108898947791072&lt;/a&gt;&lt;/p&gt;</content><author><name>by 王勤晓</name></author><summary type="html">软件开发工作者经常会听到MVC，MVP，MVVM框架。这些MVX框架，不是具体的工程技术，而是一种软件设计的模式，将软件项目的开发从逻辑上分层，体现了软件工程中模块化和解藕的思想。</summary></entry><entry><title type="html">《黑客与画家》句抄</title><link href="http://localhost:4000/jekyll/update/2019/08/10/hackers_and_painters_notes.html" rel="alternate" type="text/html" title="《黑客与画家》句抄" /><published>2019-08-10T16:30:00+08:00</published><updated>2019-08-10T16:30:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/08/10/hackers_and_painters_notes</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/08/10/hackers_and_painters_notes.html">&lt;p&gt;本周没有新的阅读。接&lt;a href=&quot;https://wangqinxiao.github.io/jekyll/update/2019/07/28/why_nerds_are_unpopular_notes.html&quot;&gt;上篇&lt;/a&gt;继续整理下Hackers and Painters（黑客与画家）一书的句子。&lt;/p&gt;

&lt;p&gt;《黑客与画家》是该书中的一篇文章。本文不讨论该文思想，只记录几个有所启发的句子。&lt;/p&gt;

&lt;p&gt;1.&lt;code class=&quot;highlighter-rouge&quot;&gt;The only external test is time. Over time, beautiful things tend to thrive, and ugly things tend to get discarded. Unfortunately, the amounts of time involved can be longer than human lifetimes.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;任何事物的评判都需要一个标准，学术圈靠论文、职场靠KPI、相亲靠高富帅。作者认为黑客的工作更像艺术，难以有客观标准，唯有时间能检验出真理／美，但问题是时间往往会很长。时间的筛选虽然漫长，但无疑都是经典的普世文化。每个人都逃不过时间的宿命，如果能留下点什么有价值的痕迹，不枉此生；如果不能，在有限的时间里阅读时间长河留下的礼物，也是一件美事。&lt;/p&gt;

&lt;p&gt;2.&lt;code class=&quot;highlighter-rouge&quot;&gt;Everyone in the sciences secretly believes that mathematicians are smarter than they are. I think mathematicians also believe this. At any rate, the result is that scientists tend to make their work look as mathematical as possible.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;人们都认为数学家是最聪明的群体。所以科学家们都喜欢把自己的工作变得像数学研究，最直接的体现就是在文章中使用公式和希腊字母。把问题数学抽象是一种归纳的方法，但不少投机之徒利用人们对数学的敬畏，故意制造障碍，以浑水摸鱼或装逼。此现象不可避免，唯一的方法就是自己掌握数学，才能识别出南郭先生。另一方面，如果真想让受众明白自己的表述，就能少用公式就少用，正如霍金曾经说过，“你多写一个公式，就会少一半的读者”。&lt;/p&gt;

&lt;p&gt;3.&lt;code class=&quot;highlighter-rouge&quot;&gt;Empathy is probably the single most important difference between a good hacker and a great one. Some hackers are quite smart, but when it comes to empathy are practically solipsists. It's hard for such people to design great software, because they can't see things from the user's point of view.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;好的黑客和优秀黑客的区别在于是否拥有“同理心”，站在用户的角度思考，才能做出优秀的产品。演讲、写作、处理人际关系、谈恋爱亦如此。&lt;/p&gt;

&lt;p&gt;4.&lt;code class=&quot;highlighter-rouge&quot;&gt;Programs should be written for people to read, and only incidentally for machines to execute.You need to have empathy not just for your users, but for your readers. It's in your interest, because you'll be one of them. Many a hacker has written a program only to find on returning to it six months later that he has no idea how it works. I know several people who've sworn off Perl after such experiences.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;程序是写给人看的，顺便让机器执行。作者认为开发者不仅要对用户有同理心，对程序的读者也要有同理心，这对开发者自己也有利，因为开发者自己也是读者的一份子。开发者如果便写的程序可读性差，可能自己几个月回来也看不懂了，更别说让其他人维护。正如乔布斯对待iPhone，不仅外表设计要完美，内部零件结构也要精益求精，正是这种态度才能造就经得起时间考验的艺术品。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.paulgraham.com/hp.html&quot;&gt;黑客与画家 原文地址&lt;/a&gt;&lt;/p&gt;</content><author><name>by 王勤晓</name></author><summary type="html">本周没有新的阅读。接上篇继续整理下Hackers and Painters（黑客与画家）一书的句子。</summary></entry><entry><title type="html">Spark程序运行过程</title><link href="http://localhost:4000/jekyll/update/2019/08/10/SparkExecuting.html" rel="alternate" type="text/html" title="Spark程序运行过程" /><published>2019-08-10T16:30:00+08:00</published><updated>2019-08-10T16:30:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/08/10/SparkExecuting</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/08/10/SparkExecuting.html">&lt;p&gt;笔者在一线碰到过很多次Spark程序崩溃的问题，而且解决起来比较费劲。纠其根本原因在于开发人员在编写Spark程序时只注重功能实现，不了解Spark程序运行过程，程序性能不佳，导致现场运行问题频发。而解决问题的过程中，由于不了解运行过程，只能靠各种尝试，解决问题效率极低。&lt;/p&gt;

&lt;p&gt;总之，不了解Spark运行原理，就没法写出可靠的Spark程序。&lt;/p&gt;

&lt;p&gt;现在就用一段Spark代码实例来一步一步解析Spark运行过程。&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# Spark program&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;textFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputPath1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;···&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;···&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;textFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputPath2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;···&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;textFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputPath3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtinone1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;union&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lines3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtinone&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtinone1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dtinone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;saveAsTextFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;···&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dtinone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;···&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;foreach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;···&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;程序运行过程如下：&lt;/p&gt;

&lt;h1 id=&quot;1-资源配置&quot;&gt;1. 资源配置&lt;/h1&gt;
&lt;p&gt;资源分配不由Spark程序决定，而是由资源管理管理器决定，如Hadoop Yarn。大数据计算需要大量硬件计算资源，资源分配决定着程序的运行性能。运行Spark程序前需要保证已配置足够可用的计算资源。资源配置参数包括，资源队列、Driver内存、Executor内存、Executor核数等等。&lt;/p&gt;

&lt;p&gt;资源配置是Spark程序运行前的准备工作，本文暂且不做深入探讨。&lt;/p&gt;

&lt;h1 id=&quot;2-创建sparkcontext&quot;&gt;2. 创建SparkContext&lt;/h1&gt;
&lt;p&gt;运行Spark程序后，第一步Driver进程启动并创建SparkContext，即该Spark程序的专属运行环境。SparkContext负责资源的申请，任务的分配和管理。相当于Spark程序运行过程中的管理者。以下步骤，除了计算，其他几乎都有SparkContext来负责。&lt;/p&gt;

&lt;p&gt;当程序运行完成后，Driver会关闭SparkContext。&lt;/p&gt;

&lt;h1 id=&quot;3-启动executor&quot;&gt;3. 启动Executor&lt;/h1&gt;
&lt;p&gt;SparkContext再创建完成后，会向资源管理器申请Executor资源，并启动Executor。&lt;/p&gt;

&lt;h1 id=&quot;4-创建dag图&quot;&gt;4. 创建DAG图&lt;/h1&gt;
&lt;p&gt;环境和资源一切就绪之后。SparkContext会根据Spark程序创建DAG（Directed Acycle graph，有向无环图），即反应所有RDD之间依赖关系的图。&lt;/p&gt;

&lt;p&gt;本文示例的程序DAG图大致如下：（本图比较简易，默认RDD没有分区。基本元素单位应该是是RDD的分区）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/DAG.jpg&quot; alt=&quot;DAG&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-触发job&quot;&gt;3. 触发Job&lt;/h1&gt;
&lt;p&gt;根据DAG图，一次RDD的Action触发一次Job（计算作业）。例如，示例代码中的saveAsTextFile就会触发一次Job。该Job包括对应RDD上的各种操作。&lt;/p&gt;

&lt;h1 id=&quot;4-分配stagetaskset&quot;&gt;4. 分配Stage（TaskSet）&lt;/h1&gt;
&lt;p&gt;Job可进一步分为Stage。分配原则为根据DAG图，Job流程从后向前，遇到宽依赖，则将当前的流程分为一个Stage。各个Stage一般并非只有线性关系，还有嵌套、并行关系。&lt;/p&gt;

&lt;p&gt;本文代码例子的Stag划分如图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/JOB.jpg&quot; alt=&quot;JOB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一个Stage对应一个TaskSet，即包含多个Task的集合。每个Task对于一个RDD（分区）的操作。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/TASK.jpg&quot; alt=&quot;TASK&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;6-提交stagetaskset&quot;&gt;6. 提交Stage（TaskSet）&lt;/h1&gt;
&lt;p&gt;提交Stage，也就是提交TaskSet。DAGScheduler通过TaskScheduler接口提交TaskSet给各个Executor，并以多线程的形式执行。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/TASK-SET.jpg&quot; alt=&quot;TASK-SET&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;7-执行task&quot;&gt;7. 执行Task&lt;/h1&gt;
&lt;p&gt;TaskScheduler构建一个TaskSetManager的实例来管理一个TaskSet的生命周期，跟踪每一个task，如果task失败，负责重试task直到达到task重试次数的最多次数。&lt;/p&gt;

&lt;p&gt;一个TaskSet在Executor中执行结束后，其结果会返回给DAGScheduler。如果得到TaskSet执行失败的信息，则会重新动态分配该Task到其他节点执行，直到重试次数的最多次数（根据笔者经验，应该是默认4次，如果继续失败，则程序崩溃）。&lt;/p&gt;

&lt;p&gt;[参考资料]：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/liuxiangke0210/article/details/79687240&quot;&gt;https://blog.csdn.net/liuxiangke0210/article/details/79687240&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>by 王勤晓</name></author><summary type="html">笔者在一线碰到过很多次Spark程序崩溃的问题，而且解决起来比较费劲。纠其根本原因在于开发人员在编写Spark程序时只注重功能实现，不了解Spark程序运行过程，程序性能不佳，导致现场运行问题频发。而解决问题的过程中，由于不了解运行过程，只能靠各种尝试，解决问题效率极低。</summary></entry><entry><title type="html">大数据处理二剑客之Spark</title><link href="http://localhost:4000/jekyll/update/2019/08/04/Hadoop&Spark1.html" rel="alternate" type="text/html" title="大数据处理二剑客之Spark" /><published>2019-08-04T17:25:00+08:00</published><updated>2019-08-04T17:25:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/08/04/Hadoop&amp;Spark1</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/08/04/Hadoop&amp;Spark1.html">&lt;p&gt;进入大数据领域，Hadoop和Spark是最先要了解的两个技术，至少目前看来是这样的。&lt;/p&gt;

&lt;p&gt;虽然一直在用它们，但对其没有系统的认识，甚至把二者混为一谈。现在梳理一下。&lt;/p&gt;

&lt;p&gt;要了解一项技术，首先思考它是干什么用的？&lt;code class=&quot;highlighter-rouge&quot;&gt;大数据领域的工作包括&quot;数据处理&quot;和&quot;数据分析&quot;&lt;/code&gt;。数据处理似食材准备，数据分析似烹饪过程。准备食材包括买菜、洗菜、切菜、腌制等过程，为下一步烹饪做准备；数据处理包括数据收集、存储、清洗、转换、组合等动作，方便我们进行下一步数据分析。&lt;code class=&quot;highlighter-rouge&quot;&gt;Hadoop和Spark就是数据处理这一阶段（大数据分析也有涉及，但以处理为主）的关键两项技术，二者有重合，但各司其职.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这篇文章梳理Spark。首先，&lt;code class=&quot;highlighter-rouge&quot;&gt;一句话描述Spark：最受欢迎的大数据计算框架&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;Spark是加州大学伯克利分校的AMP实验室开发的类似Hadoop MapReduce的通用并行框架，诞生于2009年，2013年成为了Aparch基金项目。Spark是MapReduce的替代方案，而且兼容HDFS、Hive，可融入Hadoop的生态系统，以弥补MapReduce的不足。&lt;/p&gt;

&lt;h1 id=&quot;spark-vs-hadoop&quot;&gt;Spark VS Hadoop&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://wangqinxiao.github.io/jekyll/update/2019/07/28/Hadoop&amp;amp;Spark.html&quot;&gt;大数据处理二剑客之Hadoop&lt;/a&gt;一文梳理了Hadoop生态系统，Hadoop已有MapReduce计算框架，为什么又要用Spark作为大数据计算框架呢？&lt;/p&gt;

&lt;p&gt;答案是：因为&lt;code class=&quot;highlighter-rouge&quot;&gt;Spark比Hadoop MapReduce更快&lt;/code&gt;。Spark比MapReduce更快的原因是，Hadoop MapReduce直接读写存储设备硬盘（HDFS），而&lt;code class=&quot;highlighter-rouge&quot;&gt;Spark基于内存进行计算&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;Spark和Hadoop MaoReduce计算框架一样，是分布式架构。其特点是在数据计算的过程中，把中间结果缓存在内存中。在进行大量数据计算时，直接从内存中读取数据，这要比从硬盘中读取快很多，速度优势明显。&lt;/p&gt;

&lt;p&gt;但是，基于内存的计算同样也会带来缺点。&lt;code class=&quot;highlighter-rouge&quot;&gt;与Hadoop MapReduce相比，Spark的缺点是不稳定&lt;/code&gt;。内存毕竟有限，成本也高，如果数据量过大的话，容易造成内存溢出的问题，从而导致计算过程崩溃。Hadoop MapReduce写的程序虽然慢，但是总会算出结果。而Spark写的程序常常由于数据量过大、内存不够或者计算资源配置不合理，导致崩溃（如Lost stages等等）。笔者在一线使用Spark进行数据处理，程序崩溃是最头疼的事情。&lt;/p&gt;

&lt;p&gt;鉴于Spark基于内存计算而导致的速度快的优点和不稳定的缺点。在大数据项目的计算框架技术选型时，需要&lt;code class=&quot;highlighter-rouge&quot;&gt;综合考虑数据量、业务的时间要求、可用计算资源&lt;/code&gt;。一般在&lt;code class=&quot;highlighter-rouge&quot;&gt;数据处理阶段（原始数据量一般较大）的过程中用Hadoop MapReduce&lt;/code&gt;，在&lt;code class=&quot;highlighter-rouge&quot;&gt;数据分析（数据已经过清洗，数据量一般较小）的过程中使用Spark&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;Spark已经融入Hadoop系统，可支持Hadoop Yarn资源管理，HDFS进行数据存储。&lt;/p&gt;

&lt;h1 id=&quot;spark组成部分&quot;&gt;Spark组成部分&lt;/h1&gt;

&lt;p&gt;Spark的主要组件有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SparkCore：Spark核心计算组件，实现分布式计算。它是我们最常用到的组件。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;＊ SparkSQL：Spark Sql 是Spark来操作结构化数据的程序包，可以让我使用Hive SQL语句的方式来查询和处理数据。&lt;/p&gt;

&lt;p&gt;SparkStreaming： 是Spark提供的对实时数据进行流式计算的组件。&lt;/p&gt;

&lt;p&gt;MLlib：提供常用机器学习算法的实现库。&lt;/p&gt;

&lt;p&gt;GraphX：提供一个分布式图计算框架，能高效进行图计算。&lt;/p&gt;

&lt;h1 id=&quot;spark相关概念&quot;&gt;Spark相关概念&lt;/h1&gt;

&lt;p&gt;这部分梳理我们最常用到的Spark Core的运行原理。Spark程序在运行过程中，涉及到的概念包括Application、Driver、Executer、Job、Stage、RDD等。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Application：编写的Spark应用程序就是一个Application。&lt;/li&gt;
  &lt;li&gt;Driver：Driver运行Application的Main()函数并创建SparkContext，即应用程序的运行环境。SparkContext负责分布式Cluster间的通信、任务分配管理等。通常SparkContext即代表Driver。当Executor部分运行完毕后，Driver负责将SparkContext关闭&lt;/li&gt;
  &lt;li&gt;Excuter：执行器。一个Excuter代表一个进程，负责计算任务，并将结果存在内存或者磁盘上。Excuter越多，说明进程越多，执行速度也就更快。&lt;/li&gt;
  &lt;li&gt;Job：Job是一个计算作业，由一个或着多个任务集组成。一次Spark Action，例如ReduceByKey就会催生一次计算作业，行成一个Job。一个Job包含多个RDD及作用于相应RDD上的各种Operation。&lt;/li&gt;
  &lt;li&gt;Stage：Stage是一个任务集对应的调度阶段。每个Job会被拆分很多组Task，每组任务被称为Stage（或者TaskSet）。&lt;/li&gt;
  &lt;li&gt;Task：任务被送到某个Executor上的工作任务;单个分区数据集上的最小处理流程单元.&lt;/li&gt;
  &lt;li&gt;RDD：弹性分布式数据集（Resilient Distributed Datasets，RDD），Spark的一种数据对象，是Spark的基本计算单元。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[参考资料]：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cnblogs.com/xia520pi/p/8609938.html&quot;&gt;https://www.cnblogs.com/xia520pi/p/8609938.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://baike.baidu.com/item/SPARK/2229312?fr=aladdin&quot;&gt;https://baike.baidu.com/item/SPARK/2229312?fr=aladdin&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/liuxiangke0210/article/details/79687240&quot;&gt;https://blog.csdn.net/liuxiangke0210/article/details/79687240&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>by 王勤晓</name></author><summary type="html">进入大数据领域，Hadoop和Spark是最先要了解的两个技术，至少目前看来是这样的。</summary></entry><entry><title type="html">《为什么书呆子不受欢迎》句抄</title><link href="http://localhost:4000/jekyll/update/2019/07/28/why_nerds_are_unpopular_notes.html" rel="alternate" type="text/html" title="《为什么书呆子不受欢迎》句抄" /><published>2019-07-28T19:54:00+08:00</published><updated>2019-07-28T19:54:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/07/28/why_nerds_are_unpopular_notes</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/07/28/why_nerds_are_unpopular_notes.html">&lt;p&gt;最近在看硅谷创业教父Paul Graham（保罗·格雷厄姆）的Hackers and Painters（黑客与画家），一本收录其博客的文集，讲述其对计算机文化的思考。&lt;/p&gt;

&lt;p&gt;断断续续，每周末抽出时间来读，只读了三分之一，没有记笔记，也不求甚解。回想这种读书方法是不正确的，这本书虽不是传世经典，需熟读百遍才能其义自见，但只囫囵吞枣的读一遍还是不够的。起码有个读书笔记，加上自己的思考，想想如何学以致用，再不济也要抄几个会心的句子。&lt;/p&gt;

&lt;p&gt;此前读的几篇，自然是没有笔记，但遇到不错的句子还是随手划了记号。今日先总结一篇，权当好好读书的开始。&lt;/p&gt;

&lt;h3 id=&quot;why-nerds-are-unpopuler-为什么书呆子不受欢迎&quot;&gt;Why Nerds Are Unpopuler 为什么书呆子不受欢迎&lt;/h3&gt;

&lt;p&gt;本文讲述为什么Nerds（搞技术的书呆子）不受欢迎？从Nerds本身、人类社会学、家长、学校等方面都做了分析，最后把教育制度狠狠的批判了一番。&lt;/p&gt;

&lt;p&gt;1.&lt;code class=&quot;highlighter-rouge&quot;&gt;Another reason kids persecute nerds is to make themselves feel better. When you tread water, you lift yourself up by pushing water down. Likewise, in any social hierarchy, people unsure of their own position will try to emphasize it by maltreating those they think rank below. I've read that this is why poor whites in the United States are the group most hostile to blacks.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;其他孩子欺负Nerds的一个原因是通过打压别人，抬高自己，从而从心理上获得平衡。在人类社会中，往往对自己的地位不够自信的人才会欺凌比自己阶层低的人，底层人民狗咬狗，大狗咬小狗。比如，在美国最敌视黑人的是穷的白人。对于个人而言，避免自己称为被欺凌的对象，或者靠欺凌获得心理安慰的“穷白人”，只能是往更高的阶层爬升。&lt;/p&gt;

&lt;p&gt;2.&lt;code class=&quot;highlighter-rouge&quot;&gt;Like a politician who wants to distract voters from bad times at home, you can create an enemy if there isn't a real one. By singling out and persecuting a nerd, a group of kids from higher in the hierarchy create bonds between themselves. Attacking an outsider makes them all insiders. This is why the worst cases of bullying happen with groups.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;小孩子们欺负Nerds的另一个原因是为了合群。形成一个团体最有效的办法是找到一个共同的敌人，对外同仇敌忾，内部才显得团结。政客往往会用这一招来获得国内民众的支持，最近的特朗普在2020大选之前打压华为，少不了这方面的考虑。这招够损，但实用。&lt;/p&gt;

&lt;p&gt;3.&lt;code class=&quot;highlighter-rouge&quot;&gt;Bullying was only part of the problem. Another problem, and possibly an even worse one, was that we never had anything real to work on. Humans like to work; in most of the world, your work is your identity. And all the work we did was pointless, or seemed so at the time.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;校园霸凌还有一个原因，就是闲的没事干，吃饱了撑着找找茬。作者对现在教育制度大家批判，认为都是家长和学校太懒，学生在学校做的事情没有意义。作者观点似乎太偏激，只提出了问题，没给出有效的解决方案，这里不多加讨论。我记下这句话的原因是因为他提到：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;人其实都是喜欢工作的，在大多数情况下，工作就是个人的身份。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果我们讨厌工作，觉得工作很苦很累，往往不是因为工作强度大、时间长，而是因为觉得工作没有意义，没有收获感。对于成年人来说，工作是生活的主题，并影响甚至决定家庭、感情等其他各个方面。因此，对的职业规划显得更加重要。&lt;/p&gt;

&lt;p&gt;本文完。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.paulgraham.com/nerds.html&quot;&gt;hy Nerds Are Unpopuler 原文地址&lt;/a&gt;&lt;/p&gt;</content><author><name>by 王勤晓</name></author><summary type="html">最近在看硅谷创业教父Paul Graham（保罗·格雷厄姆）的Hackers and Painters（黑客与画家），一本收录其博客的文集，讲述其对计算机文化的思考。</summary></entry><entry><title type="html">大数据处理二剑客之Hadoop</title><link href="http://localhost:4000/jekyll/update/2019/07/28/Hadoop&Spark.html" rel="alternate" type="text/html" title="大数据处理二剑客之Hadoop" /><published>2019-07-28T15:44:00+08:00</published><updated>2019-07-28T15:44:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/07/28/Hadoop&amp;Spark</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/07/28/Hadoop&amp;Spark.html">&lt;p&gt;进入大数据领域，Hadoop和Spark是最先要了解的两个技术，至少目前看来是这样的。&lt;/p&gt;

&lt;p&gt;虽然一直在用它们，但对其没有系统的认识，甚至把二者混为一谈。现在梳理一下。&lt;/p&gt;

&lt;p&gt;要了解一项技术，首先思考它是干什么用的？&lt;code class=&quot;highlighter-rouge&quot;&gt;大数据领域的工作包括&quot;数据处理&quot;和&quot;数据分析&quot;&lt;/code&gt;。数据处理似食材准备，数据分析似烹饪过程。准备食材包括买菜、洗菜、切菜、腌制等过程，为下一步烹饪做准备；数据处理包括数据收集、存储、清洗、转换、组合等动作，方便我们进行下一步数据分析。&lt;code class=&quot;highlighter-rouge&quot;&gt;Hadoop和Spark就是数据处理这一阶段（大数据分析也有涉及，但以处理为主）的关键两项技术，二者有重合，但各司其职.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这篇文章先梳理Hadoop。首先，&lt;code class=&quot;highlighter-rouge&quot;&gt;一句话描述Hadoop：大数据分析处理领域的分布式系统基础架构&lt;/code&gt;。Hadoop核心思想是分布式，即将任务分到多台计算机上进行处理。&lt;/p&gt;

&lt;h1 id=&quot;起源受启发于google&quot;&gt;起源：受启发于Google&lt;/h1&gt;

&lt;p&gt;Hadoop的诞生比Spark要早。可追溯到2004年，两个程序员Doug Cutting和Mike Cafarella受Google Lab 开发的Map/Reduce和Google File System(GFS)的启发，开始实施最初的版本（称为HDFS和MapReduce），最初Hadoop只与网页索引有关，用于处理海量网页数据进行搜索。随后由Apache基金会支持开发，逐渐发展为一个大数据分析处理领域的分布式系统基础架构。&lt;/p&gt;

&lt;p&gt;该项目的创建者，Doug Cutting解释Hadoop的得名 ：“这个名字是我孩子给一个棕黄色的大象玩具命名的“.&lt;/p&gt;

&lt;p&gt;Hadoop的主要组成部分是HDFS和MamReduce。&lt;/p&gt;

&lt;h1 id=&quot;hdfs分布式文件系统&quot;&gt;HDFS：分布式文件系统&lt;/h1&gt;
&lt;p&gt;HDFS（Hadoop Distributed File System，Hadoop分布式文件系统，顾名思义是用于存储数据的系统，与传统的分级文件系统（使用目录组织文件）相比，其一样可以增、删、改、查，HDFS的不同之处是分布式存储。&lt;/p&gt;

&lt;p&gt;分布式存储指的是存储在HDFS中的文件被分成块。HDFS有两个关键概念：NameNode和DataNode，NameNode可以控制所有文件的操作，DataNode用于存储文件。分布式文件系统设计的优势在于可支持海量数据de的快速存储和查询等操作。&lt;/p&gt;

&lt;h1 id=&quot;mapreduce分布式计算框架&quot;&gt;MapReduce：分布式计算框架&lt;/h1&gt;
&lt;p&gt;MapReduce，顾名思义其核心是Map（影射）函数和Reduce（化简、规约）函数。&lt;/p&gt;

&lt;p&gt;Map函数遍历集合里的每个目标对其应用同一个操作。再用烹饪的例子，在准备食材时需要洗菜，把茄子、辣椒、黄瓜……都洗一遍，逐个清洗这一过程就是Map，集合对象是各种食材，同一操作就是清洗。&lt;/p&gt;

&lt;p&gt;Reduce函数遍历集合里的每个目标将其综合称为一个结果。还是烹饪的例子，菜洗好后，开始做大杂烩，锅里加入茄子、加入辣椒、加入黄瓜……最终得到一盆大杂烩。加入各种食材这一过程就是Reduce，将多种食材归约为最重的一个结果———大杂烩菜。&lt;/p&gt;

&lt;p&gt;MapReduce这两个核心函数并不新鲜，其内容不仅是这二个函数（其他如Group，Sort等），而是一种编程模型，是一种方法，抽象理论。MapReduce借鉴了语言Lisp的函数式编程设计（什么是函数式编程，新文再议），体现了大数据处理过程中分而治之的思想。分布式是分而治之思想的实践，MapReduce专为分布式计算设计。&lt;/p&gt;

&lt;p&gt;除了HDFS和MapReduce，Hadoop还包括以下模块：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Hadoop Common：支持其他Hadoop模块的通用工具。&lt;/li&gt;
  &lt;li&gt;Hadoop YARN：一种作业调度和集群资源管理的框架。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Apache中其他Hadoop相关的项目包括：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Ambari：一种用于提供、管理和监督Apache Hadoop集群的基于Web UI的且易于使用的Hadoop管理工具。&lt;/li&gt;
  &lt;li&gt;Avro：一种数据序列化系统。&lt;/li&gt;
  &lt;li&gt;Cassandra：一种无单点故障的可扩展的分布式数据库。&lt;/li&gt;
  &lt;li&gt;Chukwa：一种用于管理大型分布式系统的数据收集系统。&lt;/li&gt;
  &lt;li&gt;HBase：一种支持存储大型表的结构化存储的可扩展的分布式数据库。&lt;/li&gt;
  &lt;li&gt;Hive：一种提供数据汇总和特定查询的数据仓库。&lt;/li&gt;
  &lt;li&gt;Mahout：一种可扩展的机器学习和数据挖掘库（Scala语言实现，可结合Spark后端）。&lt;/li&gt;
  &lt;li&gt;Pig：一种高级的数据流语言且支持并行计算的执行框架（2017年发布的最新版本0.17.0是添加了Spark上的Pig应* 用）。&lt;/li&gt;
  &lt;li&gt;Spark：一种用于Hadoop数据的快速通用计算引擎。Spark提供一种支持广泛应用的简单而易懂的编程模型，包括* ETL（ Extract-Transform-Load）、机器学习、流处理以及图计算。&lt;/li&gt;
  &lt;li&gt;Tez：一种建立在Hadoop YARN上数据流编程框架，它提供了一个强大而灵活的引擎来任意构建DAG* （Directed-acyclic-graph）任务去处理用于批处理和交互用例的数据。&lt;/li&gt;
  &lt;li&gt;ZooKeeper：一种给分布式应用提供高性能的协同服务系统。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;那么问题，既然Hadoop生态家族这么庞大，我们为什么要选择Spark作为对于大数据进行数据分析和数据挖掘的基本计算框架？欲知后事如何，请听下回分解。&lt;/p&gt;

&lt;p&gt;[参考资料]：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.51cto.com/xpleaf/2080181&quot;&gt;https://blog.51cto.com/xpleaf/2080181&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://baike.baidu.com/item/Hadoop/3526507&quot;&gt;https://baike.baidu.com/item/Hadoop/3526507&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>by 王勤晓</name></author><summary type="html">进入大数据领域，Hadoop和Spark是最先要了解的两个技术，至少目前看来是这样的。</summary></entry><entry><title type="html">2019 FLAG</title><link href="http://localhost:4000/jekyll/update/2019/07/21/myflag.html" rel="alternate" type="text/html" title="2019 FLAG" /><published>2019-07-21T15:44:00+08:00</published><updated>2019-07-21T15:44:00+08:00</updated><id>http://localhost:4000/jekyll/update/2019/07/21/myflag</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/07/21/myflag.html">&lt;p&gt;2019已过半，依然在华为。泰国回国后，CEA解散，换了小组，近期即将又要被调到新部门。被动的状态很危险，不做出改变，温水煮亲蛙，只会越来越被动。最近有朋友在头条让我去，虽托辞暂时没有跳槽意愿，但其实更多是因为没有积累，信心不足。常怀忧患意识，多为自己计量，才能在工作中掌握主动权，做到游刃有余，不至于狼狈不堪。&lt;/p&gt;

&lt;p&gt;预想如果跳槽，还需做好三方面的准备：&lt;/p&gt;

&lt;h1 id=&quot;1-技术&quot;&gt;1. 技术&lt;/h1&gt;

&lt;p&gt;技术是职场的立身之本。&lt;/p&gt;

&lt;p&gt;技术需要形成体系，兼顾广度和深度。当前做的是大数据、机器学习领域，一要理解各种算法思想，如分类、聚类、预测、评估等常用数据分析算法，及其业务场景下应用和延伸；二要熟悉落地工程，如Spark、Hadoop、SQL、Python、Scala，及其相互关系，技术架构和应用经验。&lt;/p&gt;

&lt;p&gt;每有体会，要常写博客，帮助自己梳理技术知识脉络。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Flag：每周一篇技术博客输出.&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-视野&quot;&gt;2. 视野&lt;/h1&gt;

&lt;p&gt;靠技术可以混口饭吃，但要更上一层楼，视野必不可少。&lt;/p&gt;

&lt;p&gt;计算机世界的发展方向？互联网的下一个风口？君子生财之道？常思考，多读书。每有所得，要总结记录下来。&lt;/p&gt;

&lt;p&gt;知道如何使用技术驱动商业价值，甚至改变世界，才是一个熟练操作工和XX师的区别。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Flag：每周一篇文化博客输出.&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-证书&quot;&gt;3. 证书&lt;/h1&gt;

&lt;p&gt;权威机构颁发的证书是证明自己实力的重要方式。&lt;/p&gt;

&lt;p&gt;面试由于时间和经历的有限，很难通过几个小时展示自己。高含金量的证书则可以为履历增彩不少，少了很多口舌，增加不少信心。&lt;/p&gt;

&lt;p&gt;大数据领域有 Cloudera 认证体系较为权威，可立为目标，以考促学。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Flag：取得一CCA证书。&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;—————————————————————————&lt;/p&gt;

&lt;p&gt;先就这样吧，Flag不要立的太多！&lt;/p&gt;</content><author><name>by 王勤晓</name></author><summary type="html">2019已过半，依然在华为。泰国回国后，CEA解散，换了小组，近期即将又要被调到新部门。被动的状态很危险，不做出改变，温水煮亲蛙，只会越来越被动。最近有朋友在头条让我去，虽托辞暂时没有跳槽意愿，但其实更多是因为没有积累，信心不足。常怀忧患意识，多为自己计量，才能在工作中掌握主动权，做到游刃有余，不至于狼狈不堪。</summary></entry><entry><title type="html">有序的故事</title><link href="http://localhost:4000/jekyll/update/2018/04/08/concordy.html" rel="alternate" type="text/html" title="有序的故事" /><published>2018-04-08T23:52:00+08:00</published><updated>2018-04-08T23:52:00+08:00</updated><id>http://localhost:4000/jekyll/update/2018/04/08/concordy</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/04/08/concordy.html">&lt;p&gt;虽然已经离开有序很久了，但当今天听到有序解散了，还是特别悲伤，就像失去了一位亲人。&lt;/p&gt;

&lt;p&gt;当初作为一个迷茫的实习生进入有序，在那里开始了我的代码之路，而且工作颇受赏识，终于找到了自己职业的方向和自信，所以一直心怀感动。毕业后经过利弊权衡，最终还是选择了抱大公司的大腿，因此自己也一直因未能跟培养我的公司一起成长而耿耿于怀，心生羞愧。&lt;/p&gt;

&lt;p&gt;对于有序我有一种很特别的情感，我很思念它。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/concordya.jpg&quot; alt=&quot;concordya&quot; /&gt;
&lt;img src=&quot;http://localhost:4000/assets/images/concordya1.jpg&quot; alt=&quot;concordya1&quot; /&gt;
&lt;img src=&quot;http://localhost:4000/assets/images/concordya2.jpg&quot; alt=&quot;concordya2&quot; /&gt;
&lt;img src=&quot;http://localhost:4000/assets/images/concordya3.jpg&quot; alt=&quot;concordya3&quot; /&gt;&lt;/p&gt;</content><author><name>by 王勤晓</name></author><summary type="html">虽然已经离开有序很久了，但当今天听到有序解散了，还是特别悲伤，就像失去了一位亲人。</summary></entry><entry><title type="html">华为留个记号</title><link href="http://localhost:4000/jekyll/update/2018/03/22/mark.html" rel="alternate" type="text/html" title="华为留个记号" /><published>2018-03-22T23:25:00+08:00</published><updated>2018-03-22T23:25:00+08:00</updated><id>http://localhost:4000/jekyll/update/2018/03/22/mark</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/03/22/mark.html">&lt;p&gt;入职华为已两月有余，现在还是在各种培训。上月暂时被派遣回部门，没有能闲扯的人，有些抑郁。这个月硬装培训倒是嘻嘻哈哈，乐呵的不行。明天就要回深圳部门了，不知未来具体要做什么？看邮件已经被分到了算法团队，会做大数据分析方向。数学一直是我的弱项，进入此领域最担心的问题胜任不了，目前还在强补。&lt;/p&gt;

&lt;p&gt;以前好不容易找到一个属于自己的方向，Web前端开发，艺术与技术的结合，当时感觉简直就是为我开设的职位，在有序又遇到了完美的团队和BOSS，干的不能再顺心了。后来遇到了论文，回校待了一年，慢慢的有序北大帮也解散了，毕业后因为离开的太久我也不好意思再开口回去。另一方面，自己也想去大公司看一看，所以最终毕业后的第一份工作便选择了南下深圳入菊厂。&lt;/p&gt;

&lt;p&gt;现在看来，菊厂做Web前端开发机会不大，做业务扯皮看人脸色肯定无法忍受，做算法好像更偏研究应用，代码诗人可能要荒废了。大公司不免程序繁琐，效率慢，况且华为文化有时感觉跟自己八字不合。所以这段时间常常在思考自己是回归IT继续当一个快乐自由的小码农，还是先在华为观望观望。以前冲动吃过亏，这次应该会选择后者，至少观望半年吧。&lt;/p&gt;

&lt;p&gt;又进入了迷茫期，不知道未来会是怎样？先留个记号。祝愿未来的自己不管职业怎样，首先能正直、能进步、能正确认识自己，再也不做那无是无非的糊涂人。&lt;/p&gt;</content><author><name>by 王勤晓</name></author><summary type="html">入职华为已两月有余，现在还是在各种培训。上月暂时被派遣回部门，没有能闲扯的人，有些抑郁。这个月硬装培训倒是嘻嘻哈哈，乐呵的不行。明天就要回深圳部门了，不知未来具体要做什么？看邮件已经被分到了算法团队，会做大数据分析方向。数学一直是我的弱项，进入此领域最担心的问题胜任不了，目前还在强补。</summary></entry><entry><title type="html">思念北京</title><link href="http://localhost:4000/jekyll/update/2018/03/16/beijing.html" rel="alternate" type="text/html" title="思念北京" /><published>2018-03-16T00:00:00+08:00</published><updated>2018-03-16T00:00:00+08:00</updated><id>http://localhost:4000/jekyll/update/2018/03/16/beijing</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/03/16/beijing.html">&lt;p&gt;毕业南下工作后，总是常常思念北京。思念燕园和中关村，思念故宫和胡同，思念长城和香山，思念玩滑板喝酒的日子，思念在有序心无旁骛地写代码，甚至开始思念那拥挤的地铁、凛冽的北风。&lt;/p&gt;

&lt;p&gt;其实，想一想，主要还是思念那里的人吧。&lt;/p&gt;</content><author><name>by 王勤晓</name></author><summary type="html">毕业南下工作后，总是常常思念北京。思念燕园和中关村，思念故宫和胡同，思念长城和香山，思念玩滑板喝酒的日子，思念在有序心无旁骛地写代码，甚至开始思念那拥挤的地铁、凛冽的北风。</summary></entry></feed>
