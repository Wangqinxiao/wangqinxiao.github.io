---
layout: post
title: 决策树算法 ID3, C4.5, CART
sub-title: 决策树算法之一
date: 2019-09-07 18:30 +0800
categories: jekyll update
---

决策树通过不断的特征二分决策，给出样本的类别。在这一过程中，有三个核心步骤：1.节点（特征，也称属性）的选择及节点顺序。2.节点如何划分。3.剪枝，剔除多余的分支，形成最终的树。决策树经典算法ID3，C4.5和CART的主要区别在第一个步骤，即特征的选择和使用顺序上用到的算法不同。

## ID3-基于信息增益
ID3（Iterative Dichotomiser 3，迭代二分器三代）由罗斯昆（J.R.Quinlan）1975年在悉尼大学提出了ID3算法。该算法基于香农（Shannon C E.）在信息论中提出的信息熵的概念。

>信息熵表示信息量的度量，熵越大，表示可能发生的结果越多，不确定性越高。在分类场景中，对于一堆样本数据，如果信息熵高说明种类多，纯度越低，相反则种类少，纯度越高。对于特征数据，信息熵越小说明该特征分类后的纯度越高，对分类系统越有效。

为了更直观的理解，可将信息熵转化为信息增益。ID3算法将特征的信息增益作为特征选取顺序的依据。

>信息增益表示某个特征能够为分类带来多少“信息”。信息增益越高，表示该特征为分类提供的信息越多。如计算瓜蒂形状的信息增益 ＝ 样本数据总的信息熵 － 瓜蒂分类后的信息熵。

信息熵，信息增益的公式如下：
![INFORMATIONH]({{ site.url }}/assets/images/INFORMATIONH.png)

关于信息熵和信息增益，推荐[https://www.jianshu.com/p/69dbb042a0e3](https://www.jianshu.com/p/69dbb042a0e3)中例子，很直观。

ID3使用信息增益来进行特征的选择，存在的问题是，属性值多的指标（如身份证号）往往信息增益会大，因此在选择时对于多属性值的特征有偏好。

## C4.5－基于信息增益率
罗斯昆在自己的ID3算法基础上进行扩展，提出了C4.5算法。C4.5也是基于信息熵的概念，与ID3不同是，使用了信息增益率。信息增益率在评估特征时，把属性值的数量也考虑了进去，从而避免了属性值过多影响判断的问题。

特征属性值数目越多，则IV(a)的值越大，信息增益率越小，一定程度上削弱了“对属性值多的特征”的偏好，但是同时也可能出现“对属性值少的特征”的偏好的问题。

## C4.5－基于基尼指数

CART（Classification And Regression Tree，分类回归树）Leo Breiman, Jerome Friedman, Richard Olshen与Charles Stone于1984年提出。CART和ID3、C4.5算法不同的之处在于：
1. ID3、C4.5算法都是基于信息熵来进行划分节点选取，主要用于分类问题，分类和回归问题都可以使用。本文直讨论分类。
2. 在节点划分时，只做二元划分。比如特征颜色取值红、黄、蓝，ID3和C4.5直接就划分为红、黄、蓝三个子类，而CART会划分为｛红｝和｛黄、蓝｝，或者｛红，黄｝和｛蓝｝，穷举所有组合。这样的好处是可以避免特征属性值数目多少的偏好问题。但准确率会有所降低。
3. CART划分的标准是使用基尼指数。当样本越不均匀，基尼指数越小。因此，我们可以先选取划分后基尼指数最小的特征（最能够区分样本）进行划分。

基尼指数公式如下：
![GINI]({{ site.url }}/assets/images/GINI.SVG)

例如，一个节点N划分后，子节点a中类0为1个，类1为5个，子节点b中类0为4个，类1为1个，则Gini(N
) ＝ |a|/|N|*Gini(a) + |b|/|N|*Gini(b) = 6/11*(1-((1/6)*(1/6)+(5/6)*(5/6))) + 5/11*(1-((4/5)*(4/5)+(1/5)*(1/5)))


---------本文结束


［参考文档］

1. [https://www.jianshu.com/p/69dbb042a0e3](https://www.jianshu.com/p/69dbb042a0e3)
2. [https://zhuanlan.zhihu.com/p/27313529](https://zhuanlan.zhihu.com/p/27313529)
3. 《数据挖掘导论》



