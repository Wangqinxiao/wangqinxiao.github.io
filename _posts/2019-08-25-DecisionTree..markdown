---
layout: post
title: 数据挖掘算法－决策树
sub-title: 随机森林的基础
date: 2019-09-01 21:30 +0800
categories: jekyll update
---

决策树（Decision Tree）是十大经典数据挖掘／机器学习算法，用于分类和回归。本文主要讨论分类。

决策树算法用于将样本进行分类（通常二分类）。例如，著名的西瓜例子，根据瓜蒂形状、瓜皮颜色、敲瓜声音等特征，反复进行二分决策（瓜蒂卷的是好瓜，否则看瓜皮-瓜皮深的是好瓜，否则再听声音......），从而分类好瓜还是坏瓜。该算法决策的流程画出来像一颗倒置得树，因此得名“决策树”。

决策树算法在分类领域应用广泛，并再此基础上衍生了随机森林等经典数据挖掘算法。包含策树的典型算法有ID3、C4.5、CART等。ID3和C4.5用于分类，CART可用于分类与回归。具体算法另有文章讨论，本文只讨论决策树的核心思想。

### 决策树构建

决策树算法的关键在于树（即分类规则）的构建，其核心步骤包括：
1. 节点（特征）的选择及节点（特征）顺序。（第一步，先判断瓜蒂形状、瓜皮颜色还是敲瓜声音；第二步......）
2. 节点（特征）如何分裂。(根据瓜蒂形状如何进行先一步判断，例如瓜蒂卷，是好瓜，否则看瓜皮......)。
3. 剪枝。去掉多余的叶结点（分类结果），使其回退到父结点（特征），甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。

步骤1、2涉及到具体的算法，ID3，C4.5，CART各不相同：ID3使用信息增益，C4.5使用信息增益比，CART使用基尼指数。具体问题另有文章分别讨论。

步骤3可分为预剪枝和后剪枝，其目的是简化树，使其具有更好的泛化能力，避免过拟合。

### 预剪枝
预剪枝的思想是在构造决策树的同时进行剪枝，即在创建之前，先计算当前的划分是否能提升模型的泛化能力，如果不能的话，就不再创建分支，直接给出叶子节点（分类结果）。常见的预剪枝方法有：
1. 当决策树达到一定的深度的时候，停止分支；
2. 当到达当前节点的样本数量小于某个阈值的时候，停止分支；
3. 计算决策树每一次分裂对测试集的准确度是否提升，当没有提升或者提升程度小于某个阈值的时候，则停止分支；

### 后剪枝
后剪枝是在决策树生长完成之后，对树进行剪枝，得到简化版的决策树。常见的后剪枝方法有：
1. 判断分支的熵的增加量是否小于某一阈值。如果是，则剪枝。
2. 用测试集进行判断，如果某分支剪掉后的准确率有所提升或者没有降低，就剪枝。

### 优点和局限

优点：
* 较随机森林可解释性，性能好。

缺点：
* 容易过拟合。
* 样本特征独立，忽略了特征之间的相关性。


---------本文结束


［参考文档］ 
1. [https://www.cnblogs.com/luban/p/9412339.html](https://www.cnblogs.com/luban/p/9412339.html)
2. [https://blog.csdn.net/Oscar6280868/article/details/86158170](https://blog.csdn.net/Oscar6280868/article/details/86158170)

