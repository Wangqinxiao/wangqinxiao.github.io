---
layout: post
title: 数据挖掘算法－随机森林
sub-title: 常用的分类算法之一
date: 2019-08-24 11:30 +0800
categories: jekyll update
---

随机森林（Random Forest，简称RF）是十大经典数据挖掘／机器学习算法，用于分类和回归。本文主要讨论分类。

### 应用场景

了解一个算法，应该从其应用开始。

RF算法用于将样本进行分类（通常二分类）。例如，笔者在帮助运营商识别潜在离网用户（目的是将用户分为将要离网OR不离网两类）中就使用了RF算法。

RF算法的使用分为训练和推理两个阶段。

训练阶段，输入训练数据集，即各种指标＋标签，如“用户类型，使用时长，网络质量等等 ＋ 是否离网标签“，经过RF算法训练，输出模型（发现的特征规律）。训练后的模型可反复使用。

推理阶段，输入样本数据，经过训练阶段输出的RF模型识别，得到样本类别。

RF算法在分类领域应用广泛。

### 发展历史

随机森林算法是基于决策树算法改进而来。上世纪八十年代Breiman等人发明了决策树算法，通过反复二分数据进行分类，例如通过西瓜数据判断好瓜坏瓜，先看瓜皮颜色深还是浅，然后看瓜蒂是否卷曲，直到得到分类结果。决策树算法这里暂不深入讲解。

2001年Breiman把多棵决策树组合成随机森林。随机森林顾名思义，该森林里面有很多的决策树，构造树的数据选择是随机的。进行分类时，输入样本进入森林，森林中的每一棵决策树对样本进行判断，看看这个样本应该属于哪一类，得票数最多的一类为最终分类结果。

与决策树相比，随机森林在运算量没有显著提高的前提下提高了预测精度(其随机性解决过拟合问题)。

### 工作原理

训练：RF使用多棵决策树，在构建每棵决策树时，对训练集进行随机且有放回地采样（bootstrap sample）。

推理：RF训练出的模型包括多棵决策树，每棵决策树都是一个分类器，那么对于一个输入样本，N棵树会有N个分类结果。根据分类投票结果，将得票数最多的类别指定为最终输出，或者输出样本属于每个类别的概率。

这种使用Boostrap采样方法，结合几个模型降低泛化误差的技术思想（避免过拟合），称为Bagging（Bootstrap aggregating）。


### 优点和局限

优点：
* 在训练阶段，随机选择数据行和列，对于多特征、不平衡、特征遗失数据，效果良好。可不用做特征选择。
* （较神经网络）运算量低，（较决策树）精度高。
* 可给出特征重要性（决策树也可以）。

缺点：
* 随机森林已经被证明在某些噪音（存在着错误或异常(偏离期望值)的数据）较大的分类或者回归问题上会过拟合。
* 随机可能导致很多相似的决策树，影响真实结果。
* 不适合处理特征较少的数据，结果可能会不好。
* 比单棵决策树慢。


总之，随机森林是一种适合处理多维（多特征）数据、不平衡、特征遗失数据的优秀分类和回归算法。


---------本文结束


［参考文档］

1. [https://blog.csdn.net/Sakura55/article/details/81413036#11-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%8E%86%E5%8F%B2](https://blog.csdn.net/Sakura55/article/details/81413036#11-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%8E%86%E5%8F%B2)
2. [https://www.jianshu.com/p/d4b32cccd747](https://www.jianshu.com/p/d4b32cccd747)

